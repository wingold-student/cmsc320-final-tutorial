{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Computer Science versus Business Management Introductory Course Professors Reviews and Their Trends Over Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "William Ingold, Erik Kelemen, Ashish Manda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project conducts a comprehensive analysis of the perceptions, grades, and reviews of computer science and business management undergraduate students at the University of Maryland, College Park. To achieve these means, we applied various data scraping, data processing, machine learning, and data visualization and analysis techniques we learned throughout the semester, and hope to demonstrate a deep understanding of data science techniques and the data science lifecycle. We also employed natural language processing and statistical analysis to draw conclusions and answer key questions. We will illustrate the major trends in student perception of core classes and professors (how favorable certain professors are, or how hard a class may be), and we will draw generalized conclusions about the trends we observe over time (is a major getting easier? more likable?). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project was sparked by interest in a question that all undergraduate students, at one point or another, face: How do my peers experience the core courses that all prospective students in my major are required to take? We hope that, by aggregating and conducting analysis of reviews and data available on major course-review websites such as PlanetTerp and RateMyProfessor, we would be able to grasp a data informed perspective to answer this question. Erik, one of the group members, is also a dual degree undergraduate student, enrolled in both the Computer Science and Business schools at UMD, and was interested in comparing student perceptions between these two majors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Querying and requests of pages\n",
    "import requests\n",
    "\n",
    "# Parsing and handling HTML elements\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Storage and manipulation of data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Used to check for the existance of files\n",
    "from os import path\n",
    "\n",
    "# Utilities\n",
    "from itertools import chain\n",
    "import collections\n",
    "import re\n",
    "import uuid\n",
    "\n",
    "# Database and data storage\n",
    "import pickle\n",
    "import csv\n",
    "import sqlite3\n",
    "from sqlite3 import Error\n",
    "\n",
    "# Selenium lets us load pages more natively, and can interact with the page\n",
    "# NOTE: You must have Selenium downloaded in order to scrape RateMyProfessor\n",
    "# This means having Chrome, Firefox, or any other browser supported downloaded\n",
    "# and installed. It also requires a driver to be downloaded.\n",
    "# SEE: https://www.selenium.dev/documentation/en/selenium_installation/installing_webdriver_binaries/\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "# CHANGE THIS TO A DRIVER LOCATION\n",
    "# OR HAVE IT IN YOUR ENVIRONMENT PATH\n",
    "selenium_driver_path = './bin/geckodriver.exe' \n",
    "\n",
    "# For handling the time & dates for reviews\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# Handling errors in try blocks\n",
    "import traceback\n",
    "\n",
    "# Graphs and Visualization\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# TODO: Keep NLTK?\n",
    "# NlTK Libraries\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "## Only need to be run once\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# SpaCy\n",
    "## If using conda, do conda install -c conda-forge spacy\n",
    "# import spacy\n",
    "import string # for punctuation list\n",
    "\n",
    "# For ML \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Determine if we should utilize data storage\n",
    "should_store_data = True\n",
    "\n",
    "# Determines if we should use Scrape RMP\n",
    "should_scrape_rmp = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Storage: Setup Databases to Hold Review Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have created two separate database systems for RateMyProfessors and PlanetTerp accordingly to hold data scraped from both of these sites. We have generic database functionality such as checking if all the data has been scraped for a certain professor. We can also retrieve important characteristics including professor stats, reviews and course grades through pandas dataframes. In addition, based on these characteristics, we can also insert and retrieve review sentiments for each professor. In the RateMyProfessors database, we created tables to store data for professor statistics, reviews, review tags, and professor tags. In the PlanetTerp database, we created tables to store data for statistics, reviews, and grades."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Storage Part 1: Generic Database Functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup database presets\n",
    "db_filepath = './data/db/'\n",
    "bmgt_rmp_db_filepath = db_filepath + 'bmgt_rmp.db'\n",
    "cmsc_rmp_db_filepath = db_filepath + 'cmsc_rmp.db'\n",
    "\n",
    "bmgt_pt_db_filepath = db_filepath + 'bmgt_pt.db'\n",
    "cmsc_pt_db_filepath = db_filepath + 'cmsc_pt.db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_connection(db_file):\n",
    "    \"\"\"Create a connection to the provided database file.\n",
    "    \n",
    "    Args:\n",
    "        db_file: A string holding the filepath to a database.\n",
    "    \"\"\"\n",
    "    \n",
    "    conn = None\n",
    "    \n",
    "    if should_store_data:\n",
    "        try:\n",
    "            conn = sqlite3.connect(db_file)\n",
    "            return conn\n",
    "        except Error as e:\n",
    "            print(e)\n",
    "\n",
    "    return conn\n",
    "\n",
    "\n",
    "def execute_create_command(conn, sql_command, params=()):\n",
    "    \"\"\"Executes the provided sql_command on the provided database.\n",
    "    \n",
    "    Args:\n",
    "        conn: The connection object to the database.\n",
    "        sql_command: A string containing the SQL command.\n",
    "        params: A tuple of potential parameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        c = conn.cursor()\n",
    "        c.execute(sql_command, params)\n",
    "        \n",
    "    except Error as e:\n",
    "        print(e)\n",
    "    \n",
    "    \n",
    "def execute_insert_command(conn, table_name, column_list, params=()):\n",
    "    \"\"\"Executes the provided sql_command on the provided database.\n",
    "    \n",
    "    Args:\n",
    "        conn: The connection object to the database.\n",
    "        sql_command: A string containing the SQL command.\n",
    "        params: A tuple of potential parameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Question mark for each value to be filled, don't want a trailing comma\n",
    "    question_marks = \"?,\" * (len(column_list) - 1)\n",
    "    question_marks = question_marks + \"?\"\n",
    "    \n",
    "    column_names = \",\".join(column_list)\n",
    "    \n",
    "    insert_sql = \"\"\"INSERT INTO {table_name} (\n",
    "                                {column_names}\n",
    "                           )\n",
    "                           VALUES({question_marks})\n",
    "                           \"\"\".format(table_name=table_name, \n",
    "                                      question_marks=question_marks,\n",
    "                                      column_names=column_names)\n",
    "    \n",
    "    try:\n",
    "        c = conn.cursor()\n",
    "        c.execute(insert_sql, params)\n",
    "        conn.commit()\n",
    "        \n",
    "        return c.lastrowid\n",
    "    except Error as e:\n",
    "        print(e)\n",
    "        \n",
    "        \n",
    "def execute_query_command(conn, sql_command, params=()):\n",
    "    \"\"\"Executes the provided sql_command on the provided database.\n",
    "    \n",
    "    Args:\n",
    "        conn: The connection object to the database.\n",
    "        sql_command: A string containing the SQL command.\n",
    "        params: A tuple of potential parameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        c = conn.cursor()\n",
    "        c.execute(sql_command, params)\n",
    "        \n",
    "        return c.fetchall()\n",
    "    \n",
    "    except Error as e:\n",
    "        print(e)\n",
    "        \n",
    "\n",
    "def is_professor_scraped(db_conn, professor_name):\n",
    "    \"\"\"Returns if the professor's RateMyProfessors page has been scraped already.\n",
    "    \n",
    "    Args:\n",
    "        db_conn: Connection object to the appropriate database.\n",
    "        professor_name: String holding the professor's name.\n",
    "    \"\"\"\n",
    "    \n",
    "    if should_store_data:\n",
    "        sql_command = \"\"\"SELECT\n",
    "                            full_name\n",
    "                        FROM\n",
    "                            professor_stats ps\n",
    "                        WHERE\n",
    "                            full_name LIKE ?\"\"\"\n",
    "\n",
    "        params=('%'+professor_name+'%',)\n",
    "\n",
    "        result = execute_query_command(db_conn, sql_command, params)\n",
    "\n",
    "        return len(result) != 0\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def insert_dataframe_into_db(db_conn, df, table_name, column_headers=None):\n",
    "    \"\"\"Inserts all rows of a given dataframe to the database's table.\n",
    "    \n",
    "    Args:\n",
    "        db_conn: Connection object to a database.\n",
    "        df: Pandas DataFrame object containing data to insert.\n",
    "        table_name: String holding a table name to insert into ('reviews' or 'professor_stats')\n",
    "    \"\"\"\n",
    "    \n",
    "    if should_store_data:\n",
    "        if column_headers is None:\n",
    "            column_list = list(df.columns)\n",
    "        else:\n",
    "            column_list = column_headers\n",
    "\n",
    "        for idx, row in df.iterrows():\n",
    "            execute_insert_command(db_conn, table_name, column_list, tuple(row.array))\n",
    "\n",
    "\n",
    "def get_professor_stats_from_db(db_conn, professor):\n",
    "    \"\"\"Reads the professor_stats table into a pandas dataframe and returns it.\"\"\"\n",
    "    \n",
    "    sql_query = \"\"\"SELECT * FROM professor_stats WHERE full_name LIKE ?\"\"\"\n",
    "    \n",
    "    return pd.read_sql_query(sql_query, db_conn, params=[professor])\n",
    "\n",
    "def get_professor_reviews_from_db(db_conn, professor):\n",
    "    \"\"\"Reads the reviews table into a pandas dataframe and returns it.\"\"\"\n",
    "    \n",
    "    sql_query = \"\"\"SELECT * FROM reviews WHERE full_name LIKE ?\"\"\"\n",
    "    \n",
    "    return pd.read_sql_query(sql_query, db_conn, params=[professor])\n",
    "\n",
    "def get_course_grades_from_db(db_conn, course):\n",
    "    \"\"\"Reads the grades table for a certain course into a pandas data\n",
    "    frame and returns it.\"\"\"\n",
    "    \n",
    "    if should_store_data:\n",
    "        sql_query = \"\"\"SELECT * FROM grades WHERE course LIKE ?\"\"\"\n",
    "\n",
    "        return pd.read_sql_query(sql_query, db_conn, params=[course])\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    \n",
    "def insert_review_sentiment(db_conn, review_id, sentiment_score, sentiment_label):\n",
    "    if should_store_data:\n",
    "        update_sql = \"\"\"UPDATE reviews\n",
    "                            SET sentiment_score = ?,\n",
    "                                sentiment_ground_label = ?\n",
    "                        WHERE review_id LIKE ?\"\"\"\n",
    "\n",
    "        try:\n",
    "            c = db_conn.cursor()\n",
    "            c.execute(update_sql, (sentiment_score, sentiment_label, review_id))\n",
    "            db_conn.commit()\n",
    "\n",
    "        except Error as e:\n",
    "            print(e)\n",
    "        \n",
    "    \n",
    "def insert_all_review_sentiment_labels(db_conn, review_sentiment_df):\n",
    "    if should_store_data:\n",
    "        for row in review_sentiment_df.itertuples():\n",
    "            insert_review_sentiment(db_conn, row.review_id, row.sentiment_score, row.sentiment_ground_label)\n",
    "\n",
    "def get_review_sentiment(db_conn, review_id):\n",
    "    sql_command = \"\"\"SELECT sentiment_score FROM reviews WHERE review_id LIKE ?\"\"\"\n",
    "    \n",
    "    result = execute_query_command(db_conn, sql_command, (review_id,))\n",
    "    \n",
    "    if len(result) == 0:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return result[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Storage Part 2: RateMyProfessor Specific Database Functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rmp_tables(rmp_conn):\n",
    "    \"\"\"Create the stats and review tables for RateMyProfessors data.\n",
    "    \n",
    "    Args:\n",
    "        rmp_conn: Connection object to a RateMyProfessors database.\n",
    "    \"\"\"\n",
    "    \n",
    "    stats_table = \"\"\" CREATE TABLE IF NOT EXISTS professor_stats (\n",
    "                        id INTEGER PRIMARY KEY,\n",
    "                        first_name TEXT NOT NULL,\n",
    "                        last_name TEXT NOT NULL,\n",
    "                        full_name TEXT NOT NULL UNIQUE ON CONFLICT IGNORE,\n",
    "                        page_exists INTEGER NOT NULL,\n",
    "                        rating REAL,\n",
    "                        take_again REAL,\n",
    "                        difficulty REAL,\n",
    "                        rating_count INTEGER\n",
    "                    ) \"\"\"\n",
    "    \n",
    "    review_table = \"\"\" CREATE TABLE IF NOT EXISTS reviews (\n",
    "                        id INTEGER PRIMARY KEY,\n",
    "                        review_id TEXT NOT NULL UNIQUE ON CONFLICT IGNORE,\n",
    "                        full_name TEXT NOT NULL,\n",
    "                        course TEXT NOT NULL,\n",
    "                        date INTEGER NOT NULL,\n",
    "                        body TEXT NOT NULL,\n",
    "                        thumb_up INTEGER,\n",
    "                        thumb_down INTEGER,\n",
    "                        rating REAL NOT NULL,\n",
    "                        difficulty REAL NOT NULL,\n",
    "                        sentiment_score REAL,\n",
    "                        sentiment_ground_label INTEGER\n",
    "                   ) \"\"\"\n",
    "    \n",
    "    review_tags_table = \"\"\"CREATE TABLE IF NOT EXISTS review_tags (\n",
    "                            id INTEGER PRIMARY KEY,\n",
    "                            review_id TEXT NOT NULL,\n",
    "                            tag_name TEXT NOT NULL\n",
    "                        )\"\"\"\n",
    "    \n",
    "    review_meta_table = \"\"\"CREATE TABLE IF NOT EXISTS meta_tags (\n",
    "                            id INTEGER PRIMARY KEY,\n",
    "                            review_id TEXT NOT NULL,\n",
    "                            meta_name TEXT NOT NULL,\n",
    "                            value TEXT NOT NULL\n",
    "                        )\"\"\"\n",
    "    \n",
    "    professor_tags_table = \"\"\"CREATE TABLE IF NOT EXISTS professor_tags (\n",
    "                                id INTEGER PRIMARY KEY,\n",
    "                                full_name TEXT NOT NULL,\n",
    "                                tag_name\n",
    "                        )\"\"\"\n",
    "    \n",
    "    execute_create_command(rmp_conn, stats_table)\n",
    "    execute_create_command(rmp_conn, review_table)\n",
    "    execute_create_command(rmp_conn, review_tags_table)\n",
    "    execute_create_command(rmp_conn, review_meta_table)\n",
    "    execute_create_command(rmp_conn, professor_tags_table)\n",
    "\n",
    "    \n",
    "def get_rmp_review_grade(rmp_conn, review_id):\n",
    "    sql_query = \"\"\"SELECT value, review_id FROM meta_tags WHERE review_id LIKE ? AND meta_name LIKE '%Grade%'\"\"\"\n",
    "    \n",
    "    return pd.read_sql_query(sql_query, rmp_conn, params=[review_id])\n",
    "\n",
    "\n",
    "def get_rmp_review_tags(rmp_conn, review_id):\n",
    "    sql_query = \"\"\"SELECT review_id, tag_name FROM review_tags WHERE review_id LIKE ?\"\"\"\n",
    "    \n",
    "    return pd.read_sql_query(sql_query, rmp_conn, params=[review_id])\n",
    "\n",
    "\n",
    "def get_rmp_review_all_meta(rmp_conn, review_id):\n",
    "    sql_query = \"\"\"SELECT review_id, meta_name, value FROM meta_tags WHERE review_id LIKE ?\"\"\"\n",
    "    \n",
    "    return pd.read_sql_query(sql_query, rmp_conn, params=[review_id])\n",
    "\n",
    "\n",
    "def get_rmp_data_for_all_ids(rmp_filepath, review_df, query_func):\n",
    "    \"\"\"Given a DataFrame full of review_ids, query the database for the requested data\n",
    "    for each review_id and create a dataframe full of the data. This will be easier to\n",
    "    merge with other review dataframes, since they'll have matching review_ids.\n",
    "    \n",
    "    Args:\n",
    "        rmp_filepath: String holding the filepath to the RMP database.\n",
    "        review_df: A pandas dataframe that should hold review_ids\n",
    "        query_func: A RMP database query function, i.e. get_rmp_review_grade,\n",
    "            get_rmp_review_tags, get_rmp_all_meta\n",
    "            \n",
    "    Returns:\n",
    "        A dataframe with at least a review_id column and columns of requested data.\n",
    "    \"\"\"\n",
    "    \n",
    "    rmp_conn = create_connection(rmp_filepath)\n",
    "    data_df = pd.DataFrame()\n",
    "    \n",
    "    try:\n",
    "        data = review_df['review_id'].apply(lambda review_id: query_func(rmp_conn, review_id)).tolist()\n",
    "        data_df = pd.concat(data)\n",
    "    except Exception as e:\n",
    "        print(\"Type error: \" + str(e))\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        rmp_conn.close()\n",
    "        \n",
    "    return data_df \n",
    "\n",
    "\n",
    "def get_rmp_prof_tags(rmp_conn, full_name):\n",
    "    sql_query = \"\"\"SELECT tag_name FROM professor_tags WHERE full_name LIKE ?\"\"\"\n",
    "    \n",
    "    return pd.read_sql_query(sql_query, rmp_conn, params=[full_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if should_store_data:\n",
    "    # Create the CMSC and BMGT database with the two tables\n",
    "    cmsc_rmp_db = create_connection(cmsc_rmp_db_filepath)\n",
    "    bmgt_rmp_db = create_connection(bmgt_rmp_db_filepath)\n",
    "\n",
    "    create_rmp_tables(cmsc_rmp_db)\n",
    "    create_rmp_tables(bmgt_rmp_db)\n",
    "\n",
    "    # Close for now, will reopen when writing to them\n",
    "    cmsc_rmp_db.close()\n",
    "    bmgt_rmp_db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Storage Part 3: PlanetTerp Database Functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pt_tables(pt_conn):\n",
    "    \"\"\"Create the stats and review tables for RateMyProfessors data.\n",
    "    \n",
    "    Args:\n",
    "        pt_conn: Connection object to a PlanetTerp database.\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Keep track of grade distribution in this table?\n",
    "    stats_table = \"\"\" CREATE TABLE IF NOT EXISTS professor_stats (\n",
    "                        id INTEGER PRIMARY KEY,\n",
    "                        first_name TEXT NOT NULL,\n",
    "                        last_name TEXT NOT NULL,\n",
    "                        full_name TEXT NOT NULL UNIQUE ON CONFLICT IGNORE,\n",
    "                        page_exists INTEGER NOT NULL,\n",
    "                        slug TEXT,\n",
    "                        review_count INTEGER NOT NULL,\n",
    "                        rating REAL,\n",
    "                        type TEXT\n",
    "                    ) \"\"\"\n",
    "    \n",
    "    # TODO: Review id format? <professor last name>-<#> ?\n",
    "    review_table = \"\"\" CREATE TABLE IF NOT EXISTS reviews (\n",
    "                        id INTEGER PRIMARY KEY,\n",
    "                        review_id TEXT NOT NULL UNIQUE ON CONFLICT IGNORE,\n",
    "                        full_name TEXT NOT NULL,\n",
    "                        course TEXT NOT NULL,\n",
    "                        date INTEGER NOT NULL,\n",
    "                        body TEXT NOT NULL,\n",
    "                        rating INTEGER NOT NULL,\n",
    "                        expected_grade TEXT,\n",
    "                        sentiment_score REAL,\n",
    "                        sentiment_ground_label INTEGER\n",
    "                   ) \"\"\"\n",
    "    \n",
    "    grades_table = \"\"\" CREATE TABLE IF NOT EXISTS grades (\n",
    "                            id INTEGER PRIMARY KEY,\n",
    "                            course TEXT NOT NULL,\n",
    "                            semester INTEGER,\n",
    "                            grade TEXT,\n",
    "                            count INTEGER NOT NULL\n",
    "                    )\"\"\"\n",
    "    \n",
    "    execute_create_command(pt_conn, stats_table)\n",
    "    execute_create_command(pt_conn, review_table)\n",
    "    execute_create_command(pt_conn, grades_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if should_store_data:\n",
    "    # Create the CMSC and BMGT database with the two tables\n",
    "    cmsc_pt_db = create_connection(cmsc_pt_db_filepath)\n",
    "    bmgt_pt_db = create_connection(bmgt_pt_db_filepath)\n",
    "\n",
    "    create_pt_tables(cmsc_pt_db)\n",
    "    create_pt_tables(bmgt_pt_db)\n",
    "\n",
    "    # Close for now, will reopen when writing to them\n",
    "    cmsc_pt_db.close()\n",
    "    bmgt_pt_db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End Data Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Accomplished section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection Part 1: Grabbing Introductory Course Professors From UMD.io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We utilize the APIâ€™s from both UMD.io and PlanetTerp to collect data regarding the professors and courses they offer. Since we are analyzing the trends in introductory courses, we are collecting statistics for the CS intro courses CMSC131, CMSC132, CMSC216, and CMSC250. Meanwhile, on the business side, we are collecting data for intro courses BMGT110, BMGT220, BMGT221, and BMGT230. We start by grabbing a general list of professors and the courses they offer at UMD. Then, we are able to map each professor to the introductory courses mentioned above that they offer by creating a dictionary of professor to courses. Lastly, we can separate the professors into their specific department to create a list of CMSC and BMGT professors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base API url for UMD.io\n",
    "professors_url = \"https://api.umd.io/v1/professors\"\n",
    "\n",
    "# Base API url for PlanetTerp\n",
    "pt_courses_url = \"https://api.planetterp.com/v1/course\"\n",
    "\n",
    "# The filepaths for the files to hold professor information\n",
    "cmsc_professor_names_filepath = './data/cmsc_professor_names.csv'\n",
    "bmgt_professor_names_filepath = './data/bmgt_professor_names.csv'\n",
    "\n",
    "# Determines if we've created these already\n",
    "have_cmsc_professors = path.exists(cmsc_professor_names_filepath)\n",
    "have_bmgt_professors = path.exists(bmgt_professor_names_filepath)\n",
    "\n",
    "\n",
    "# Courses we're interseted in look at\n",
    "# TODO: What about honors?\n",
    "# TODO: Dr. Eastman has taught CMSC131, per RateMyProfessor, but wasn't given via UMD.IO\n",
    "cmsc_course_ids = [\"CMSC131\", \"CMSC132\", \"CMSC216\", \"CMSC250\"]\n",
    "bmgt_course_ids = [\"BMGT110\", \"BMGT220\", \"BMGT221\", \"BMGT230\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utilities for saving professor data to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_professor_name_data(professor_filepath):\n",
    "    \"\"\"Reads the professor names and their courses from a CSV file.\n",
    "    \n",
    "    Args:\n",
    "        professor_filepath: String holding a filepath to the professor csv file.\n",
    "        \n",
    "    Returns:\n",
    "        A dictionary of professor names to a set of courses they have taught.\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(professor_filepath, mode='r') as csv_file:\n",
    "        csv_reader = csv.DictReader(csv_file)\n",
    "        line_count = 0\n",
    "\n",
    "        professors = {}\n",
    "\n",
    "        for row in csv_reader:\n",
    "            if line_count != 0:\n",
    "                professors[row['name']] = set([course for course in row['courses'].split(' ')])\n",
    "            line_count += 1\n",
    "\n",
    "        return professors\n",
    "\n",
    "def save_professor_data(professors, filepath):\n",
    "    \"\"\"Saves the professor names and their courses to a CSV file.\n",
    "    \n",
    "    Args:\n",
    "        professors: A dictionary of professor name keys and a set of courses for values.\n",
    "    \"\"\"\n",
    "    \n",
    "    columns = ['name', 'courses']\n",
    "    try:\n",
    "        with open(filepath, 'w') as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=columns)\n",
    "            writer.writeheader()\n",
    "            \n",
    "            for name, courses in professors.items():\n",
    "                writer.writerow({'name': name, 'courses': ' '.join(courses)})\n",
    "                \n",
    "    except IOError:\n",
    "        print(\"Error in writing the CSV file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility to actually grab professors based on a list of courses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_professors_for_courses_from_umdio(course_ids):\n",
    "    \"\"\"Gets all the professors for the given course_ids from UMD.io \n",
    "    and returns a dictionary of professor to courses.\n",
    "    \n",
    "    Args:\n",
    "        course_ids: A list of course ids (e.g. ['CMSC216', CMSC250']).\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of professor to set of courses.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    professors = {}\n",
    "    \n",
    "    for course_id in course_ids:\n",
    "        params = {'course_id': course_id}\n",
    "\n",
    "        response = requests.get(professors_url, params)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "\n",
    "            for item in response.json():\n",
    "                name = item['name']\n",
    "\n",
    "                if name in professors:\n",
    "                    professors[name].add(course_id)\n",
    "                else:\n",
    "                    professors[name] = {course_id}\n",
    "\n",
    "    return professors\n",
    "\n",
    "\n",
    "def get_professors_for_courses_from_pt(course_ids):\n",
    "    \"\"\"Gets all the professors for the given course_ids from PlanetTerp\n",
    "    and returns a dictionary of professor to courses.\n",
    "    \n",
    "    Args:\n",
    "        course_ids: A list of course ids (e.g. ['CMSC216', CMSC250']).\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of professor to set of courses.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    professor_list = {}\n",
    "    \n",
    "    for course_id in course_ids:\n",
    "        params = {'name': course_id}\n",
    "\n",
    "        response = requests.get(pt_courses_url, params)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            professors = response.json().get('professors', None)\n",
    "            \n",
    "            if professors:\n",
    "                for professor in professors:\n",
    "                    if professor in professor_list:\n",
    "                        professor_list[professor].add(course_id)\n",
    "                    else:\n",
    "                        professor_list[professor] = {course_id}\n",
    "\n",
    "    return professor_list\n",
    "\n",
    "def combine_professor_dictionaries(dict_one, dict_two):\n",
    "    combined_profs = collections.defaultdict(set)\n",
    "\n",
    "    for key, val in chain(dict_one.items(), dict_two.items()):\n",
    "        combined_profs[key] = combined_profs[key].union(val)\n",
    "        \n",
    "    return combined_profs\n",
    "\n",
    "def get_all_professors_from_courses(course_ids):\n",
    "    umdio_professors = get_professors_for_courses_from_umdio(course_ids)\n",
    "    pt_professors = get_professors_for_courses_from_pt(course_ids)\n",
    "    \n",
    "    return combine_professor_dictionaries(umdio_professors, pt_professors)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grab Computer Science Professors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only query the UMD.io API if we don't have the data\n",
    "if not have_cmsc_professors or not should_store_data:\n",
    "    cmsc_professors = get_all_professors_from_courses(cmsc_course_ids)\n",
    "    \n",
    "    save_professor_data(cmsc_professors, cmsc_professor_names_filepath)\n",
    "    have_cmsc_professors = True\n",
    "else: \n",
    "    cmsc_professors = read_professor_name_data(cmsc_professor_names_filepath)\n",
    "\n",
    "    if not cmsc_professors:\n",
    "        print(\"Error response from umd.io API\")\n",
    "\n",
    "if 'Iason Filippou' in cmsc_professors:\n",
    "    cmsc_professors.pop('Iason Filippou') # A typo of Jason Filippou from the database\n",
    "    \n",
    "print(cmsc_professors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grab Business Management Professors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only query the UMD.io API if we don't have the data\n",
    "if not have_bmgt_professors or not should_store_data:\n",
    "    bmgt_professors = get_all_professors_from_courses(bmgt_course_ids)\n",
    "    \n",
    "    save_professor_data(bmgt_professors, bmgt_professor_names_filepath)\n",
    "    have_bmgt_professors = True\n",
    "else:\n",
    "    bmgt_professors = read_professor_name_data(bmgt_professor_names_filepath)\n",
    "\n",
    "    if not bmgt_professors:\n",
    "        print(\"Error response from umd.io API\")\n",
    "\n",
    "print(bmgt_professors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection Part 2: Grabbing Reviews From RateMyProfessors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to scrape the reviews for each professor and their corresponding introductory courses we need to take a look at RateMyProfessor. The website offers a list of tags used to describe each professor so we use those tags as the headers for our databases. Then we obtain the URL of each professor in order to parse their overall information such as their corresponding statistics and tags. We then utilize the Selenium browser to load all the different professor reviews which are continuously loaded since RateMyProfessors paginates the reviews via Javascript. For each review, we parse specific utilities including ratings, tags, thumb scoring, etc. Once the utilities are parsed, we obtain the overall statistics for each professor based on their reviews and store it in a dataframe. To wrap up the RateMyProfessors data collection we filter out the CMSC professors and reviews from the BMGT professors and reviews.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection Part 2.1: Setup and Utilities to Scrape and Parse Data from RateMyProfessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Collection Part 2.1.1: Setup Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data needed for requesting data from RateMyProfessor\n",
    "ratemyprofessor_url = \"https://www.ratemyprofessors.com/search.jsp\"\n",
    "params = {'queryoption':'HEADER', 'schoolID':'1270', 'queryBy':'teacherName', 'schoolName':'University+of+Maryland'}\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:82.0) Gecko/20100101 Firefox/82.0\",\n",
    "    \"Access-Control-Allow-Origin\": \"*\",\n",
    "    \"Access-Control-Allow-Headers\": \"Content-Type\",\n",
    "    \"Access-Control-Allow-Methods\": \"GET\",\n",
    "}\n",
    "\n",
    "\n",
    "# List of tags that RateMyProfessor uses to describe professors, which are used for the database and dataframes\n",
    "tag_list = ['gives_good_feedback', 'respected', 'lots_of_homework', 'accessible_outside_class',\n",
    "           'get_ready_to_read', 'participation_matters', 'inspirational',\n",
    "           'graded_by_few_things', 'test_heavy', 'group_projects', 'clear_grading_criteria', \n",
    "           'hilarious', 'beware_of_pop_quizes', 'amazing_lectures', 'lecture_heavy', 'caring',\n",
    "           'extra_credit', 'so_many_papers', 'tough_grader', 'skip_class_wont_pass']\n",
    "\n",
    "# Want to tie the code friendly tag names to what is found on a RateMyProfessor page\n",
    "text_tag_list = [' '.join(x.split('_')) for x in tag_list]\n",
    "text_tag_list.remove('skip class wont pass')\n",
    "text_tag_list.append(\"skip class? you won't pass.\")\n",
    "\n",
    "# both tag_list and text_tag_list in same order, and correspond to one another\n",
    "text_tag_dict = {text_tag_list[i]: tag_list[i] for i in range(len(text_tag_list))}\n",
    "\n",
    "# These are the column headers for a professor's overall statistics found at the top of the page\n",
    "overall_header_list = ['first_name', 'last_name', 'full_name', 'page_exists', 'rating', 'take_again', 'difficulty',\n",
    "                      'rating_count']\n",
    "\n",
    "# Review post column headers. The meta list is the row of top meta responses (like 'Grade: A-').\n",
    "review_meta_list = ['would_take_again', 'grade', 'textbook', 'online_class', 'for_credit', 'attendance']\n",
    "review_text_meta_list = [' '.join(x.split('_')) for x in review_meta_list]\n",
    "review_meta_dict = {review_text_meta_list[i]: review_meta_list[i] for i in range(len(review_meta_list))}\n",
    "        \n",
    "review_header_list = ['review_id', 'full_name', 'course', 'date', 'rating', 'difficulty', 'body',\n",
    "                      'thumb_up', 'thumb_down']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Collection Part 2.1.2: Utility Functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tags_to_dict(provided_tags):\n",
    "    \"\"\"Turns the list of text tags (e.g. skip class? you won't pass) into a dictionary\n",
    "    of approriately named tags that work for database columns and if they were present.\n",
    "    \n",
    "    Args:\n",
    "        provided_tags: A list of space separated tags scraped from the RMP page.\n",
    "        \n",
    "    Returns:\n",
    "        A dictionary of {tag: 1 or 0} on whether a tag was used to describe the professor.\n",
    "    \"\"\"\n",
    "    \n",
    "    tag_dict = {val: 0 for val in text_tag_dict.values()}\n",
    "    \n",
    "    for tag in provided_tags:\n",
    "        if tag.lower() in text_tag_dict.keys():\n",
    "            tag_dict[text_tag_dict[tag.lower()]] = 1\n",
    "            \n",
    "    return tag_dict\n",
    "\n",
    "def meta_to_dict(provided_meta):\n",
    "    \"\"\"Turns the dictionary of meta tags (e.g. Would Take Again: No) into a dictionary\n",
    "    of appropriately named tags that work for database columns and values if they were present.\n",
    "    \n",
    "    Args:\n",
    "        provided_meta: A dictionary of meta information from a review.\n",
    "        \n",
    "    Returns:\n",
    "        A dictionary of {meta: 1 or 0} on whether a meta was used on the review.\n",
    "    \"\"\"\n",
    "    \n",
    "    meta_dict = {val: 0 for val in review_meta_list}\n",
    "    \n",
    "    for meta, response in provided_meta.items():\n",
    "        value = 0\n",
    "        \n",
    "        if meta.lower() in review_meta_dict.keys():\n",
    "            \n",
    "            if response.lower() == \"yes\" or response.lower() == \"mandatory\":\n",
    "                value = 1\n",
    "                \n",
    "            if meta.lower() == \"grade\":\n",
    "                value = response\n",
    "            \n",
    "            meta_dict[review_meta_dict[meta.lower()]] = value\n",
    "            \n",
    "    return meta_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection Part 2.2: Querying RateMyProfessor and Getting the Professor's URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_rmp_professor_url(html_doc):\n",
    "    \"\"\"Finds the professor's URL on the search page and returns it.\n",
    "    \n",
    "    Args:\n",
    "        html_doc: A string containing an HTML document.\n",
    "        \n",
    "    Returns:\n",
    "        The full URL for the professor's page (if found).\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "    main_url = \"https://www.ratemyprofessors.com\"\n",
    "    prof_urls = []\n",
    "    \n",
    "    no_results = soup.find('div[class*=\"NoResultsFoundArea__StyledNoResultsFound\"]')\n",
    "    prof_list_items = soup.find_all('li', class_='listing PROFESSOR')\n",
    "    \n",
    "    # Sometimes RMP does the search differntly, so it'll be elsewhere\n",
    "    diff_location = soup.find('a', attrs={'class': lambda x: 'TeacherCard__StyledTeacherCard' in x if x else False}, href=True)\n",
    "    \n",
    "    # The professor may not be reviewed\n",
    "    if no_results is None and prof_list_items and len(prof_list_items) != 0:\n",
    "        if diff_location:\n",
    "            prof_urls.append(main_url + diff_location['href'])\n",
    "        else:\n",
    "            # Each should be from University of Maryland due to search params\n",
    "            for item in prof_list_items: \n",
    "                partial_url = item.find('a', href=True)\n",
    "\n",
    "                if partial_url:\n",
    "                    prof_urls.append(main_url + partial_url['href'])\n",
    "                        \n",
    "                        \n",
    "    return prof_urls\n",
    "    \n",
    "    \n",
    "def query_rmp_for_professor_url(professor_name, headers, params):\n",
    "    \"\"\"Queries RateMyProfessor for the professor, given the parameters and headers.\n",
    "    \n",
    "    Args:\n",
    "        professor_name: The <first name> <last name> of the professor.\n",
    "        headers: Dictionary of headers for the get request.\n",
    "        params: Dictionary of parameters for the get request.\n",
    "        \n",
    "    Returns:\n",
    "        The full URL for the professor's page after searching for it (if found).\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    params['query'] = professor_name\n",
    "    \n",
    "    response = requests.get(ratemyprofessor_url, headers=headers, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        url = find_rmp_professor_url(response.text)\n",
    "        \n",
    "        if url is not None:\n",
    "            return url\n",
    "        else:\n",
    "            print(\"Professor {name} has not been reviewed.\".format(name=professor_name))\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection Part 2.3: Parsing the Professor Overall Information (Stats and Tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rmp_prof_stats(page_text):\n",
    "    \"\"\"Parses the professor's stats from their page and returns them. Namely their overall rating, \n",
    "    how many would take again, overall difficulty and how many ratings they have on RateMyProfessor.\n",
    "    \n",
    "    Args:\n",
    "        page_text: An HTML document of the professor's page.\n",
    "        \n",
    "    Returns:\n",
    "        A dictionary containing their rating, take again percentage, difficulty rating, and rating count.\n",
    "    \"\"\"\n",
    "    \n",
    "    soup = BeautifulSoup(page_text, 'html.parser')\n",
    "    \n",
    "    rating_score = soup.select('div[class*=\"RatingValue__Numerator\"]')\n",
    "    \n",
    "    take_again = np.nan\n",
    "    difficulty = np.nan\n",
    "    \n",
    "    if rating_score is not None and rating_score[0].text != 'N/A':\n",
    "        rating_score = float(rating_score[0].text)\n",
    "    else:\n",
    "        rating_score = np.nan\n",
    "    \n",
    "    feedback_cont = soup.select('div[class*=\"TeacherFeedback__StyledTeacherFeedback\"]') #[0].select('div[class*=\"FeedbackItem__FeedbackNumber\"]')\n",
    "    \n",
    "    if feedback_cont and len(feedback_cont) > 0:\n",
    "        feedback_nums = feedback_cont[0].select('div[class*=\"FeedbackItem__FeedbackNumber\"]')\n",
    "        \n",
    "        if feedback_nums and len(feedback_nums) == 2:\n",
    "            if len(feedback_nums[0].text) > 0:\n",
    "                try:\n",
    "                    take_again = float(feedback_nums[0].text[:-1]) / 100\n",
    "                except ValueError:\n",
    "                    take_again = np.nan\n",
    "                \n",
    "            if len(feedback_nums[1].text) > 0:\n",
    "                try:\n",
    "                    difficulty = float(feedback_nums[1].text)\n",
    "                except ValueError:\n",
    "                    difficulty = np.nan\n",
    "    \n",
    "    rating_count_int = 0\n",
    "    rating_count_cont = soup.select('div[class*=\"RatingValue__NumRatings\"]') #[0].select('a')[0].text\n",
    "    \n",
    "    \n",
    "    if rating_count_cont and len(rating_count_cont) > 0:\n",
    "        rating_count_a = rating_count_cont[0].select('a')\n",
    "        \n",
    "        if rating_count_a and len(rating_count_a) > 0:\n",
    "            rating_count = rating_count_a[0].text\n",
    "            \n",
    "            try:\n",
    "                rating_count_int = int(rating_count)\n",
    "            except ValueError:\n",
    "                rating_count_int = 0\n",
    "            \n",
    "        \n",
    "    try:\n",
    "        rating_count_int = int(''.join([x for x in rating_count if x.isdigit()]))\n",
    "    except ValueError:\n",
    "        rating_count_int = 0\n",
    "        \n",
    "    stats_dict = {'rating': rating_score, 'take_again': take_again, 'difficulty': difficulty, 'rating_count': rating_count_int}\n",
    "    return stats_dict\n",
    "\n",
    "\n",
    "def get_rmp_prof_top_tags(page_text, prof_name, rmp_conn):\n",
    "    \"\"\"Parses and returns the professor's top tags.\n",
    "    \n",
    "    Args:\n",
    "        page_text: An HTML document of the professor's page.\n",
    "        \n",
    "    Returns:\n",
    "        A list of tags describing the professor.\n",
    "    \"\"\"\n",
    "    \n",
    "    soup = BeautifulSoup(page_text, 'html.parser')\n",
    "    \n",
    "    tags_df = pd.DataFrame(columns=['full_name', 'tag_name'])\n",
    "    \n",
    "    unparsed_tags = soup.select('div[class*=\"TeacherTags__TagsContainer\"]')\n",
    "    \n",
    "    if unparsed_tags and len(unparsed_tags) != 0:\n",
    "        unparsed_tags = unparsed_tags[0].select('span')\n",
    "    \n",
    "        for tag in unparsed_tags:\n",
    "            tags_df = tags_df.append({'full_name': prof_name, 'tag_name': tag.text}, ignore_index=True)\n",
    "        \n",
    "    insert_dataframe_into_db(rmp_conn, tags_df, 'professor_tags')\n",
    "\n",
    "\n",
    "def rmp_prof_overall_to_dataframe(professor_name, stats, page_exists=1):\n",
    "    \"\"\"Combines the professor's overall stats and tags into a pandas dataframe.\n",
    "    \n",
    "    Args:\n",
    "        professor_name: String holding the professor's name.\n",
    "        stats: A dictionary holding the overall stats (e.g. 'would_take_again': .83).\n",
    "        tags: A dictionary holding the tags associated with a professor (e.g. {'caring': 1}).\n",
    "        page_exists (optional, default=1): Integer boolean determining if a professor has a RMP page.\n",
    "        \n",
    "    Returns:\n",
    "        A dataframe containing the combination of professor name, stats, and tags.\n",
    "    \"\"\"\n",
    "    \n",
    "    overall_df = pd.DataFrame(columns=overall_header_list)\n",
    "    \n",
    "    first_name, last_name = professor_name.split(' ', 1)\n",
    "    overall_dict = {'first_name': first_name, 'last_name': last_name, 'full_name': professor_name, 'page_exists': page_exists}\n",
    "    \n",
    "    overall_dict.update(stats)\n",
    "    \n",
    "    overall_df = overall_df.append(overall_dict, ignore_index=True)\n",
    "    \n",
    "    return overall_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection Part 2.4: Use Selenium to Load All Professor Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_selenium():\n",
    "    \"\"\"Starts up the Selenium browser.\"\"\"\n",
    "    driver = webdriver.Firefox(executable_path='./bin/geckodriver.exe')\n",
    "    return driver\n",
    "    \n",
    "def stop_selenium(driver):\n",
    "    \"\"\"Shutdown the Selenium browser.\"\"\"\n",
    "    driver.close()\n",
    "    driver.quit()\n",
    "    \n",
    "def load_all_rmp_reviews(page_url, driver):\n",
    "    \"\"\"Loads all the reviews for a given porfessor and returns the text of all of them.\n",
    "    \n",
    "    Args:\n",
    "        page_url: The URL for the professor's page.\n",
    "        \n",
    "    Returns:\n",
    "        A string containing the HTML for all the reviews.\n",
    "    \"\"\"\n",
    "\n",
    "    driver.get(page_url)\n",
    "    \n",
    "    # RateMyProfessors has a cookies pop up that overlays the website, it needs to be closed first\n",
    "    time.sleep(1)\n",
    "    close_cookies = driver.find_elements(By.XPATH, '//button[text()=\"Close\"]')\n",
    "    \n",
    "    if close_cookies:\n",
    "        close_cookies[0].click()\n",
    "        \n",
    "    load_more = driver.find_elements(By.XPATH, '//button[text()=\"Load More Ratings\"]')\n",
    "    \n",
    "    # RateMyProfessors paginates the reviews via Javascript, so we must continually load more while the button is present\n",
    "    while load_more:\n",
    "        load_more[0].click()\n",
    "        time.sleep(1)\n",
    "        load_more = driver.find_elements(By.XPATH, '//button[text()=\"Load More Ratings\"]')\n",
    "        \n",
    "        \n",
    "    try:\n",
    "        all_reviews = driver.find_element_by_id('ratingsList').get_attribute('outerHTML')\n",
    "    except NoSuchElementException:\n",
    "        all_reviews = ''\n",
    "    \n",
    "    \n",
    "    return all_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection Part 2.5: Parsing Utilities for a Single Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_date_to_unix(date_str):\n",
    "    \"\"\"Turns the RateMyProfessor date format (e.g. Nov 23rd, 2020) into a\n",
    "    UTC timestamp. Assumes the date is already in UTC.\n",
    "    \n",
    "    Args:\n",
    "        date_str: A string containing the RateMyProfessor review date.\n",
    "        \n",
    "    Returns:\n",
    "        A UTC timestamp corresponding to the date provided.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Split into month, day, year\n",
    "    date_split = date_str.split(' ')\n",
    "    day = date_split[1]\n",
    "    \n",
    "    # Remove comma and suffix for day\n",
    "    day = day[:-3]\n",
    "    \n",
    "    # Place the day back into the list and join everything back together\n",
    "    date_split[1] = day\n",
    "    remade_date_str = (' ').join(date_split)\n",
    "    \n",
    "    # Change into UTC time\n",
    "    datetime_obj = datetime.datetime.strptime(remade_date_str, '%b %d %Y')\n",
    "    utc_time = datetime_obj.timestamp()\n",
    "    \n",
    "    return utc_time\n",
    "    \n",
    "def parse_rating_header(soup):\n",
    "    \"\"\"Parses and returns the rating header for a single review.\n",
    "    \n",
    "    Args:\n",
    "        soup: An initialized BeautifulSoup object for the professor's page.\n",
    "        \n",
    "    Returns:\n",
    "        A dictionary containing the course and date for the review.\n",
    "    \"\"\"\n",
    "    \n",
    "    rating_header = soup.select('div[class*=\"Rating__RatingInfo\"]')\n",
    "    \n",
    "    if len(rating_header) != 0:\n",
    "        course = rating_header[0].select('div[class*=\"RatingHeader__StyledClass\"]')[0].text.strip()\n",
    "        date = rating_header[0].select('div[class*=\"TimeStamp__StyledTimeStamp\"]')[0].text.strip()\n",
    "        \n",
    "        utc_time = string_date_to_unix(date)\n",
    "    else:\n",
    "        print(soup)\n",
    "    \n",
    "    return {'course': course, 'date': utc_time}\n",
    "\n",
    "def parse_meta_data(soup, review_id, rmp_conn):\n",
    "    \"\"\"Parses and returns the meta data for a single review.\n",
    "    \n",
    "    Args:\n",
    "        soup: An initialized BeautifulSoup object for the professor's page.\n",
    "        \n",
    "    Returns:\n",
    "        A dictionary containing the meta data (e.g. Would Take Again) for the review.\n",
    "    \"\"\"\n",
    "    \n",
    "    course_meta = soup.select('div[class*=\"CourseMeta__StyledCourseMeta\"]')[0]\n",
    "    review_meta_data = {}\n",
    "    \n",
    "    meta_tag_df = pd.DataFrame(columns=['review_id', 'meta_name', 'value'])\n",
    "\n",
    "    for meta_div in course_meta.select('div'):\n",
    "        meta_data = meta_div.text.split(':')\n",
    "        meta_name = meta_data[0].strip()\n",
    "        meta_value = meta_data[1].strip()\n",
    "\n",
    "        meta_tag_df = meta_tag_df.append({'review_id': review_id, 'meta_name': meta_name, 'value': meta_value}, ignore_index=True)\n",
    "        \n",
    "        \n",
    "    insert_dataframe_into_db(rmp_conn, meta_tag_df, 'meta_tags')\n",
    "\n",
    "\n",
    "def parse_rating_data(soup):\n",
    "    \"\"\"Parses and returns the rating data for a single review.\n",
    "    \n",
    "    Args:\n",
    "        soup: An initialized BeautifulSoup object for the professor's page.\n",
    "        \n",
    "    Returns:\n",
    "        A dictionary containing the rating data for the quality and difficulty for the review.\n",
    "    \"\"\"\n",
    "    \n",
    "    rating_values_text = soup.select('div[class*=\"RatingValues__StyledRatingValues\"]')[0].select('div[class*=\"RatingValues__RatingValue\"]')\n",
    "    quality = rating_values_text[0].text\n",
    "    difficulty = rating_values_text[1].text\n",
    "\n",
    "    rating_data = {'rating': quality, 'difficulty': difficulty}\n",
    "    \n",
    "    return rating_data\n",
    "\n",
    "def parse_review_tags(soup, review_id, rmp_conn):\n",
    "    \"\"\"Parses and returns the tags for a single review.\n",
    "    \n",
    "    Args:\n",
    "        soup: An initialized BeautifulSoup object for the professor's page.\n",
    "        \n",
    "    Returns:\n",
    "        A list containing the tags for the review.\n",
    "    \"\"\"\n",
    "    \n",
    "    tag_container = soup.select('div[class*=\"RatingTags__StyledTags\"]')\n",
    "    \n",
    "    review_tags_df = pd.DataFrame(columns=['review_id', 'tag_name'])\n",
    "    \n",
    "    if tag_container: # Since not all reviews add tags\n",
    "        unparsed_tags = tag_container[0].select('span')\n",
    "\n",
    "        for tag in unparsed_tags:\n",
    "            review_tags_df = review_tags_df.append({'review_id': review_id, 'tag_name': tag.text}, ignore_index=True)\n",
    "\n",
    "    insert_dataframe_into_db(rmp_conn, review_tags_df, 'review_tags')\n",
    "    \n",
    "def parse_thumb_scoring(soup):\n",
    "    \"\"\"Parses and returns the thumb scoring data for a single review.\n",
    "    \n",
    "    Args:\n",
    "        soup: An initialized BeautifulSoup object for the professor's page.\n",
    "        \n",
    "    Returns:\n",
    "        A dictionary containing the thumb scoring data for the review.\n",
    "    \"\"\"\n",
    "    \n",
    "    thumb_container = soup.select('div[class*=\"RatingFooter__StyledRatingFooter\"]')[0].select('div[class*=\"RatingFooter__HelpTotal\"]')\n",
    "\n",
    "    thumb_up = int(thumb_container[0].text.strip())\n",
    "    thumb_down = int(thumb_container[1].text.strip())\n",
    "    thumb_data = {'thumb_up': thumb_up, 'thumb_down': thumb_down}\n",
    "\n",
    "    return thumb_data\n",
    "\n",
    "def parse_review_text(soup):\n",
    "    \"\"\"Parses and returns the review body text for a single review.\n",
    "    \n",
    "    Args:\n",
    "        soup: An initialized BeautifulSoup object for the professor's page.\n",
    "        \n",
    "    Returns:\n",
    "        A string containing the review text for the review.\n",
    "    \"\"\"\n",
    "    \n",
    "    review_text = soup.select('div[class*=\"Comments__StyledComments\"]')[0].text\n",
    "    \n",
    "    return {'body': review_text}\n",
    "    \n",
    "def parse_single_rmp_review(rmp_url, review_item, courses, rmp_conn):\n",
    "    \"\"\"Parses and returns all data for a single review.\n",
    "    Namely it returns: Meta data, rating data, tags, thumb_scoring, and review text.\n",
    "    \n",
    "    Args:\n",
    "        review_item: A single review list item containing all the appropraite HTML.\n",
    "        \n",
    "    Returns:\n",
    "        A dictionary containing the meta data, rating data, tags, thumb_scoring, and review text\n",
    "        for a single review.\n",
    "    \"\"\"\n",
    "    \n",
    "    soup = BeautifulSoup(review_item, 'html.parser')\n",
    "    \n",
    "    course_and_date = parse_rating_header(soup)\n",
    "    \n",
    "    # TODO: Loses course reviews like 'CMSC131CMSC132' where students combined multiple courses they took\n",
    "    if course_and_date['course'] in courses:\n",
    "        \n",
    "        # Rating data\n",
    "        rating_data = parse_rating_data(soup)\n",
    "        \n",
    "        # Thumb Scoring\n",
    "        thumb_scoring = parse_thumb_scoring(soup)\n",
    "        \n",
    "        # Review body\n",
    "        review_text = parse_review_text(soup)\n",
    "        \n",
    "        # Generate a UUID for the review using the review's text - IMPORTANT FOR FINDING AGAIN\n",
    "        review_id = uuid.uuid5(uuid.NAMESPACE_URL, rmp_url + review_text['body'])\n",
    "        review_id_str = str(review_id)\n",
    "        \n",
    "        # Meta data - Stored in own table in the database\n",
    "        meta_data = parse_meta_data(soup, review_id_str, rmp_conn)\n",
    "        \n",
    "        # Tags - Stored in own table in the database\n",
    "        parse_review_tags(soup, review_id_str, rmp_conn)\n",
    "        \n",
    "        return {'review_id_data': {'review_id': review_id_str}, 'rating_data': rating_data, 'thumb_scoring': thumb_scoring,\n",
    "                'review_text': review_text, 'rating_header': course_and_date}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection 2.6: Parsing Utilities for an Entire RateMyProfessor Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rmp_prof_reviews(rmp_prof_url, selenium_driver, rmp_conn, prof_name, courses):\n",
    "    \"\"\"Gets all the RateMyProfessor reviews for a given professor and places into a\n",
    "    dataframe. Only grabs reviews for classes in the provided courses.\n",
    "    \n",
    "    Args:\n",
    "        rmp_prof_url: A string containing the RateMyProfessor URL for the professor.\n",
    "        prof_name: A string containing the professor's name.\n",
    "        prof_courses: List of courses to look for in the reviews.\n",
    "        \n",
    "    Returns:\n",
    "        A dataframe containing all the appropriate reviews.\n",
    "    \"\"\"\n",
    "    \n",
    "    reviews_html = load_all_rmp_reviews(rmp_prof_url, selenium_driver)\n",
    "    soup = BeautifulSoup(reviews_html, 'html.parser')\n",
    "    \n",
    "    first_name, last_name = prof_name.split(' ', 1)\n",
    "    \n",
    "    review_df = pd.DataFrame(columns=review_header_list)\n",
    "    \n",
    "    for review in soup.find_all('li'):\n",
    "        \n",
    "        if len(review.select('div[class*=\"Rating__StyledRating\"]')) != 0: # Avoid advertisement list items\n",
    "            data = parse_single_rmp_review(rmp_prof_url, str(review), courses, rmp_conn)\n",
    "\n",
    "            if data: # Since the review could be of an undesired course\n",
    "                flattened_data = {'full_name': prof_name}\n",
    "\n",
    "                for data_type, data_dict in data.items():\n",
    "                    \n",
    "                    for key, val in data_dict.items():\n",
    "                        flattened_data[key] = val\n",
    "\n",
    "                review_df = review_df.append(flattened_data, ignore_index=True)\n",
    "    \n",
    "    return review_df\n",
    "\n",
    "\n",
    "def parse_rmp_page(rmp_prof_url, headers, rmp_conn, selenium_driver, professor_name, courses):\n",
    "    \"\"\"Parses an entire RateMyProfessor professor page for overall stats & tags, and all\n",
    "    of their reviews. It will return two dataframes holding this information and insert\n",
    "    them into a database.\n",
    "    \n",
    "    Args:\n",
    "        rmp_prof_url: A string containing the RateMyProfessor URL for the professor.\n",
    "        headers: Request headers to use.\n",
    "        rmp_conn: Connection object to the RateMyProfessor database.\n",
    "        prof_name: A string containing the professor's name.\n",
    "        courses: List of courses to look for in the reviews.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple of two dataframes, (overall statistics, all the reviews).\n",
    "    \"\"\"\n",
    "    \n",
    "    rmp_prof_page = requests.get(rmp_prof_url, headers=headers)\n",
    "    \n",
    "    if rmp_prof_page.status_code == 200:\n",
    "        soup = BeautifulSoup(rmp_prof_page.text, 'html.parser')\n",
    "        \n",
    "        # Professor stats\n",
    "        stats_container = soup.select('div[class*=\"TeacherInfo__StyledTeacher\"]')[0]\n",
    "        \n",
    "        prof_stats = get_rmp_prof_stats(str(stats_container))\n",
    "        \n",
    "        get_rmp_prof_top_tags(str(stats_container), professor_name, rmp_conn)\n",
    "        \n",
    "        overall_df = rmp_prof_overall_to_dataframe(professor_name, prof_stats)\n",
    "        insert_dataframe_into_db(rmp_conn, overall_df, 'professor_stats')\n",
    "        \n",
    "        # Professor reviews\n",
    "        all_reviews_df = get_rmp_prof_reviews(rmp_prof_url, selenium_driver, rmp_conn, professor_name, courses)\n",
    "        insert_dataframe_into_db(rmp_conn, all_reviews_df, 'reviews')\n",
    "        \n",
    "        return (overall_df, all_reviews_df)\n",
    "    else:\n",
    "        print(\"Error opening the RateMyProfessor professor page\")\n",
    "        return (None, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection 2.7: Scrape and Parse All Professors Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_nonexistant_rmp_data(rmp_conn, professor):\n",
    "    \"\"\"Marks a professor as not having a page and fills professor's overall statistics dataframe\n",
    "    with empty values so that it may be placed into the database and not re-queried for later.\n",
    "    \n",
    "    Args:\n",
    "        rmp_db: Connection object to the RateMyProfessor database.\n",
    "        professor: String containing the name of the professor.\n",
    "    \"\"\"\n",
    "    \n",
    "    empty_stats = {'rating': 0, 'take_again': 0, 'difficulty': 0, 'rating_count': 0}\n",
    "    \n",
    "    # Professor stats\n",
    "    overall_df = rmp_prof_overall_to_dataframe(professor, empty_stats, page_exists=0)\n",
    "    insert_dataframe_into_db(rmp_conn, overall_df, 'professor_stats')\n",
    "    \n",
    "    return None\n",
    "    \n",
    "    \n",
    "def parse_rmp_all_professors(rmp_db_filepath, professors, provided_courses, force_scrape=False):\n",
    "    \"\"\"Scrapes and parses all professors, storing the data in a database and returning a\n",
    "    list of dataframes for stats and reviews.\n",
    "    \n",
    "    Args:\n",
    "        rmp_db_filepath: String containing the filepath to the appropriate database.\n",
    "        professors: Dictionary of professors to list of courses.\n",
    "        force_scrape (optional, default=False): Forces a scrape of RateMyProfessors even if already done.\n",
    "        \n",
    "    Returns:\n",
    "        The tuple (stats, reviews) where each is a list of dataframes.\n",
    "    \"\"\"\n",
    "    \n",
    "    all_major_stats = []\n",
    "    all_major_reviews = []\n",
    "    \n",
    "    rmp_db = create_connection(rmp_db_filepath)\n",
    "    \n",
    "    if should_scrape_rmp:\n",
    "        selenium_driver = start_selenium()\n",
    "    \n",
    "    try:\n",
    "        for professor, courses in professors.items():\n",
    "            overall_stats_df = None\n",
    "            all_reviews_df = None\n",
    "\n",
    "            # Read from database if the professor has already been scraped (only checks stats for confirmation)\n",
    "            if not force_scrape and is_professor_scraped(rmp_db, professor):\n",
    "                overall_stats_df = get_professor_stats_from_db(rmp_db, professor)\n",
    "                all_reviews_df = get_professor_reviews_from_db(rmp_db, professor)\n",
    "\n",
    "                # Keep track of the dataframes for each professor\n",
    "                all_major_stats.append(overall_stats_df)\n",
    "                all_major_reviews.append(all_reviews_df)\n",
    "\n",
    "            else:\n",
    "                # Get all the data from the professor's RateMyProfessor page\n",
    "                prof_rmp_urls = query_rmp_for_professor_url(professor, headers, params)\n",
    "\n",
    "                # If the professor has a RateMyProfessor page\n",
    "                for url in prof_rmp_urls:\n",
    "                    \n",
    "                    overall_stats_df, all_reviews_df = parse_rmp_page(url, headers, rmp_db, selenium_driver, professor, provided_courses)\n",
    "\n",
    "                    # Keep track of the dataframes for each professor\n",
    "                    all_major_stats.append(overall_stats_df)\n",
    "                    all_major_reviews.append(all_reviews_df)\n",
    "                    \n",
    "                    # So we don't query RateMyProfessor too much\n",
    "                    time.sleep(1)\n",
    "\n",
    "                else:\n",
    "                    # Used to fill the stats table to show their page doesn't exist\n",
    "                    fill_nonexistant_rmp_data(rmp_db, professor)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Type error: \" + str(e))\n",
    "        traceback.print_exc()\n",
    "\n",
    "    finally:\n",
    "        if should_store_data:\n",
    "            rmp_db.close()\n",
    "            \n",
    "        if should_scrape_rmp:\n",
    "            stop_selenium(selenium_driver)\n",
    "    \n",
    "        return (all_major_stats, all_major_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection Part 2.8: Scrape and Parse All Computer Science Professors from RateMyProfessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rmp_cmsc_stats, all_rmp_cmsc_reviews = parse_rmp_all_professors(cmsc_rmp_db_filepath, cmsc_professors, cmsc_course_ids)\n",
    "\n",
    "merged_rmp_cmsc_stats = pd.concat(all_rmp_cmsc_stats)\n",
    "merged_rmp_cmsc_reviews = pd.concat(all_rmp_cmsc_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(merged_rmp_cmsc_stats))\n",
    "merged_rmp_cmsc_stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(merged_rmp_cmsc_reviews))\n",
    "merged_rmp_cmsc_reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection Part 2.9: Scrape and Parse All Business Management Professors from RateMyProfessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rmp_bmgt_stats, all_rmp_bmgt_reviews = parse_rmp_all_professors(bmgt_rmp_db_filepath, bmgt_professors, bmgt_course_ids)\n",
    "\n",
    "merged_rmp_bmgt_stats = pd.concat(all_rmp_bmgt_stats)\n",
    "merged_rmp_bmgt_reviews = pd.concat(all_rmp_bmgt_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(merged_rmp_bmgt_stats))\n",
    "merged_rmp_bmgt_stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(merged_rmp_bmgt_reviews))\n",
    "merged_rmp_bmgt_reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection Part 3: Query and Parse Data from PlanetTerp "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we collect data from our second source, the PlanetTerp API. This API is slightly different from the RateMyProfessors website because it does not contain tags to describe each professor. Instead, we collect stats, reviews, and grade data from PlanetTerp. We parse each individual review and place it into a dictionary. We place those that correspond to the introductory courses we are looking at into a separate data frame. Then we are able to query the API to obtain the stats and reviews for a given professor placing them into separate data frames. More functionality is added on this data such as summing semester grades, accumulating course grades and returning a data frame consisting of those. As we did with RateMyProfessors, we filter out the data for the CMSC professors and reviews from the BMGT professors and reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection Part 3.1: Setup and Utilities for PlanetTerp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See https://api.planetterp.com/#get-a-professor\n",
    "planetterp_api = \"https://api.planetterp.com/v1/professor\"\n",
    "pt_header = {'Accept': 'application/json'}\n",
    "params = {'reviews': 'true'}\n",
    "\n",
    "# Stats and reviews data\n",
    "stats_columns=['first_name', 'last_name', 'full_name', 'slug', 'review_count', 'type', 'page_exists']\n",
    "review_columns=['review_id', 'full_name', 'course', 'date', 'body', 'rating', 'expected_grade']\n",
    "\n",
    "# Grade data\n",
    "grades_list = ['A+', 'A', 'A-', 'B+', 'B', 'B-', 'C+', 'C', 'C-',\n",
    "                  'D+', 'D', 'D-', 'F', 'W']\n",
    "\n",
    "grades_headers = ['course', 'semester', 'grade', 'count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pt_date_to_unix(date_str):\n",
    "    \"\"\"Takes the PlanetTerp datetime string and converts to unix time. Assumes\n",
    "    It is already in UTC timezone.\n",
    "    \n",
    "    Args:\n",
    "        date_str: String containing a date time in the format \"%Y-%m-%dT%H:%M:%S\".\n",
    "        \n",
    "    Returns:\n",
    "        A unix timestamp real representing the time passed into the function.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Format: 2020-01-01T00:00:00\n",
    "    date_time_obj = datetime.datetime.strptime(date_str, \"%Y-%m-%dT%H:%M:%S\")\n",
    "    return date_time_obj.timestamp() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection Part 3.2: Parsing PlanetTerp Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Could simply use the review JSON provided, but may not be the format we want\n",
    "def parse_pt_single_review(review, courses):\n",
    "    \"\"\"Parses a single PlanetTerp review and places it into a dictionary.\n",
    "    \n",
    "    Args:\n",
    "        review: A dictionary or JSON object holding the review data.\n",
    "        review_id: A string holding an unique id for this review.\n",
    "        courses: List of course ids to determine if review wanted.\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary holding review information.\n",
    "    \"\"\"\n",
    "    \n",
    "    review_dict = {}\n",
    "    course = review.get('course')\n",
    "    \n",
    "    if course and course in courses:\n",
    "        review_dict = {'full_name': review.get('professor'), 'course': course,\n",
    "                       'body': review.get('review'), 'expected_grade': review.get('expected_grade', np.nan),\n",
    "                       'rating': review.get('rating')}\n",
    "\n",
    "        unix_time = pt_date_to_unix(review.get('created'))\n",
    "        review_dict['date'] = unix_time\n",
    "\n",
    "        # TODO: Important for getting the right reviews later\n",
    "        review_id = uuid.uuid5(uuid.NAMESPACE_URL, planetterp_api + review_dict['body'] + str(review_dict['date']))\n",
    "        review_id_str = str(review_id)\n",
    "        \n",
    "        review_dict['review_id'] = review_id_str\n",
    "        \n",
    "    return review_dict\n",
    "    \n",
    "    \n",
    "def parse_pt_reviews(reviews, courses):\n",
    "    \"\"\"Parses all reviews from PlanetTerp, placing those that are within the desired courses\n",
    "    into a dataframe.\n",
    "    \n",
    "    Args:\n",
    "        reviews: A list of dictionaries, each dictionary representing a single review.\n",
    "        reviews_df: A dataframe to hold the reviews.\n",
    "        courses: The desired courses for which to look for in the reviews.\n",
    "        \n",
    "    Returns:\n",
    "        A dataframe containing all the reviews for a professor.\n",
    "    \"\"\"\n",
    "    \n",
    "    avg_rating = 0\n",
    "    course_count = 0\n",
    "    \n",
    "    reviews_df = pd.DataFrame(columns=review_columns)\n",
    "    \n",
    "    for review in reviews:\n",
    "        \n",
    "        review_dict = parse_pt_single_review(review, courses)\n",
    "        \n",
    "        if bool(review_dict):\n",
    "            reviews_df = reviews_df.append(review_dict, ignore_index=True)\n",
    "            avg_rating = avg_rating + review_dict['rating']\n",
    "            course_count = course_count + 1\n",
    "            \n",
    "    if course_count != 0:\n",
    "        avg_rating = float(avg_rating) / course_count\n",
    "        \n",
    "    return (reviews_df, avg_rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection Part 3.3: Querying PlanetTerp for Professors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_pt_for_professor(professor, courses):\n",
    "    \"\"\"Queries the PlanetTerp API for a given professor, gathering their stats\n",
    "    and reviews. It then returns two dataframes (stats, reviews).\n",
    "    \n",
    "    Args:\n",
    "        professor: String holding the name of the professor to query.\n",
    "        courses: List of course ids to look for in the reviews.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (stats, reviews) of dataframes holding the stats and reviews data.\n",
    "    \"\"\"\n",
    "    \n",
    "    stats_df = pd.DataFrame(columns=stats_columns)\n",
    "    reviews_df = pd.DataFrame()\n",
    "    \n",
    "    params['name'] = professor\n",
    "    \n",
    "    \n",
    "    response = requests.get(planetterp_api, headers=pt_header, params=params)\n",
    "    \n",
    "    first_name, last_name = professor.split(' ', 1)\n",
    "    prof_stats = {'first_name': first_name, 'last_name': last_name, \n",
    "                  'full_name': professor}\n",
    "    \n",
    "    # The professor may not exist in the PlanetTerp database (though this shouldn't occur)\n",
    "    if response.status_code == 200:\n",
    "        json = response.json()\n",
    "        \n",
    "        review_count = 0\n",
    "        avg_rating = None\n",
    "        reviews = json.get('reviews')\n",
    "        \n",
    "        # The professor may not have any reviews\n",
    "        if reviews:\n",
    "            review_count = len(reviews)\n",
    "            reviews_df, avg_rating = parse_pt_reviews(reviews, courses)\n",
    "            \n",
    "        stats_cont = {'slug': json.get('slug'), 'type': json.get('type'),\n",
    "                     'review_count': review_count, 'rating': avg_rating, 'page_exists': 1}\n",
    "        \n",
    "    else:\n",
    "        stats_cont = {'page_exists': 0, 'review_count': 0}\n",
    "        \n",
    "        \n",
    "    prof_stats.update(stats_cont)\n",
    "    stats_df = stats_df.append(prof_stats, ignore_index=True)\n",
    "    \n",
    "    return (stats_df, reviews_df)\n",
    "\n",
    "\n",
    "def query_pt_for_all_professors(professors, courses, db_filepath, force_query=False):\n",
    "    \"\"\"Queries PlanetTerp for all the professors provided, taking reviews that\n",
    "    correspond to the given courses, and places professor stats and \n",
    "    reviews into a database.\n",
    "    \n",
    "    Args:\n",
    "        professors: A list of strings containing professor names.\n",
    "        courses: A list of strings containing course ids.\n",
    "        db_filepath: A string holding the filepath to a database.\n",
    "        force_query (optional, default=False): Boolean to decied whether to force\n",
    "            query the PlanetTerp API.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (stats, reviews) of lists containing all dataframes for\n",
    "        each professor stats and reviews respectively.\n",
    "    \"\"\"\n",
    "    \n",
    "    all_major_stats = []\n",
    "    all_major_reviews = []\n",
    "    \n",
    "    try:\n",
    "        pt_db = create_connection(db_filepath)\n",
    "        for professor in professors:\n",
    "            \n",
    "            if not force_query and is_professor_scraped(pt_db, professor):\n",
    "                \n",
    "                stats_df = get_professor_stats_from_db(pt_db, professor)\n",
    "                reviews_df = get_professor_reviews_from_db(pt_db, professor)\n",
    "\n",
    "                # Keep track of the dataframes for each professor\n",
    "                all_major_stats.append(stats_df)\n",
    "                all_major_reviews.append(reviews_df)\n",
    "                \n",
    "            else:\n",
    "                stats_df, reviews_df = query_pt_for_professor(professor, courses)\n",
    "\n",
    "                if not stats_df.empty:\n",
    "                    all_major_stats.append(stats_df)\n",
    "                    insert_dataframe_into_db(pt_db, stats_df, 'professor_stats')\n",
    "\n",
    "                if not reviews_df.empty:\n",
    "                    all_major_reviews.append(reviews_df)\n",
    "                    insert_dataframe_into_db(pt_db, reviews_df, 'reviews')\n",
    "\n",
    "                time.sleep(1) # To give some time to the PlanetTerp API\n",
    "            \n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Type error: \" + str(e))\n",
    "        traceback.print_exc()\n",
    "        \n",
    "    finally:\n",
    "        if should_store_data:\n",
    "            pt_db.close()\n",
    "            \n",
    "        return (all_major_stats, all_major_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_semester_grades(course_df):\n",
    "    \"\"\"Sums up grades across the same course and semester and returns a\n",
    "    new dataframe containing this information. Course grades are originally\n",
    "    grouped by their section and professor, so we want to aggregate them.\n",
    "    \n",
    "    Args:\n",
    "        course_df: A dataframe holding grades per section and professor.\n",
    "        \n",
    "    Returns:\n",
    "        A dataframe where the identical courses and semesters have their\n",
    "        grades aggregated.\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.DataFrame(columns=course_df.columns)\n",
    "    semester_groups = course_df.groupby(['course', 'semester', 'grade']).sum().reset_index()\n",
    "    return semester_groups\n",
    "\n",
    "def accumulate_course_grades(course_grades_json):\n",
    "    \"\"\"Accumulates all the grades of a course into a dataframe, because course\n",
    "    grades will be separated out by their section and professor.\n",
    "    \n",
    "    Args:\n",
    "        course_grades_json: A dictionary from PlanetTerp API containing grade info\"\n",
    "        \n",
    "    Returns:\n",
    "        A dataframe where all grades of the same course and semester are accumulated.\n",
    "    \"\"\"\n",
    "    \n",
    "    course_grades_dict =  {}\n",
    "    course_grades_df = pd.DataFrame(columns=grades_headers)\n",
    "    \n",
    "    for course in course_grades_json:\n",
    "        course_grade_dict = {'semester': course['semester'], 'course': course['course']}\n",
    "        \n",
    "        for grade in grades_list:\n",
    "            course_grade_dict['grade'] = grade\n",
    "            course_grade_dict['count'] = course.pop(grade, 0)\n",
    "            \n",
    "            course_grades_df = course_grades_df.append(course_grade_dict, ignore_index=True)\n",
    "        \n",
    "    return sum_semester_grades(course_grades_df)\n",
    "    \n",
    "def query_pt_for_course_grades(courses, db_filepath, force_query=False):\n",
    "    \"\"\"Queries PlanetTerp for the grades for each course, accumulates\n",
    "    all grades of identitical courses and semesters, and places into a database.\n",
    "    \n",
    "    Args:\n",
    "        courses: A list of course ids to query the grades PlantTerp for\n",
    "        db_filepath: A database filepath to open and insert data into\n",
    "        force_query (optional, default=False): Boolean determining whether\n",
    "            PlanetTerp should be queried, regardless of database info.\n",
    "            \n",
    "    Returns:\n",
    "        A dataframe of all course, semester grades accumulated.\n",
    "    \"\"\"\n",
    "    \n",
    "    pt_db = create_connection(db_filepath)\n",
    "    \n",
    "    grades_api = 'https://api.planetterp.com/v1/grades'\n",
    "    grades_params = {'course': None}\n",
    "    \n",
    "    all_course_grades = []\n",
    "    \n",
    "    try:\n",
    "        for course in courses:\n",
    "            \n",
    "            # Check whether we've already queried for this course\n",
    "            course_grades_df = get_course_grades_from_db(pt_db, course)\n",
    "            \n",
    "            if course_grades_df.empty:\n",
    "                course_grades_df = pd.DataFrame(columns=grades_header_dict.values())\n",
    "            \n",
    "            if force_query or course_grades_df.empty:\n",
    "                grades_params['course'] = course\n",
    "                response = requests.get(grades_api, headers=pt_header, params=grades_params)\n",
    "\n",
    "                if response.status_code == 200:\n",
    "                    grade_data = response.json()\n",
    "                    \n",
    "                    # Accumulate the grade info for this course\n",
    "                    course_grades_df = accumulate_course_grades(grade_data)\n",
    "                    all_course_grades.append(course_grades_df)\n",
    "                    \n",
    "                    # Put the course grades into the database\n",
    "                    insert_dataframe_into_db(pt_db, course_grades_df, 'grades', column_headers=grades_headers)\n",
    "                    \n",
    "                    # Give the API a bit of time\n",
    "                    time.sleep(0.5)\n",
    "                    \n",
    "            else:\n",
    "                all_course_grades.append(course_grades_df)\n",
    "                \n",
    "                \n",
    "    except Exception as e:\n",
    "        print(\"Type error: \" + str(e))\n",
    "        traceback.print_exc()\n",
    "        \n",
    "    finally:\n",
    "        \n",
    "        if should_store_data:\n",
    "            pt_db.close()\n",
    "        \n",
    "        if len(all_course_grades) == 0:\n",
    "            print(\"Error getting any course grades\")\n",
    "            return None\n",
    "        \n",
    "        return pd.concat(all_course_grades)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection Part 3.4: Parse All Computer Science Professors from PlanetTerp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pt_cmsc_stats, all_pt_cmsc_reviews = query_pt_for_all_professors(cmsc_professors, cmsc_course_ids, cmsc_pt_db_filepath)\n",
    "all_pt_cmsc_grades = query_pt_for_course_grades(cmsc_course_ids, cmsc_pt_db_filepath)\n",
    "\n",
    "merged_pt_cmsc_stats = pd.concat(all_pt_cmsc_stats)\n",
    "merged_pt_cmsc_reviews = pd.concat(all_pt_cmsc_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(merged_pt_cmsc_stats))\n",
    "merged_pt_cmsc_stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(merged_pt_cmsc_reviews))\n",
    "merged_pt_cmsc_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_pt_cmsc_grades))\n",
    "all_pt_cmsc_grades.head(14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection Part 3.5: Parse All Business Management Professors from PlanetTerp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pt_bmgt_stats, all_pt_bmgt_reviews = query_pt_for_all_professors(bmgt_professors, bmgt_course_ids, bmgt_pt_db_filepath)\n",
    "all_pt_bmgt_grades = query_pt_for_course_grades(bmgt_course_ids, bmgt_pt_db_filepath)\n",
    "\n",
    "merged_pt_bmgt_stats = pd.concat(all_pt_bmgt_stats)\n",
    "merged_pt_bmgt_reviews = pd.concat(all_pt_bmgt_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(merged_pt_bmgt_stats))\n",
    "merged_pt_bmgt_stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(merged_pt_bmgt_reviews))\n",
    "merged_pt_bmgt_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_pt_bmgt_grades))\n",
    "all_pt_bmgt_grades.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection Part 4: Putting It All Together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To organize our data frames, we filter the data we collected from RateMyProfessor, PlanetTerp, and data that was consistent between both these sources. We separate the data based on major. To finalize some of the data, we combine rating counts and sum the total as well as finding the average rating for each professor from both RateMyProfessor and PlanetTerp. Each individual data frame that is created in this section is explained in the Analysis portion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Across PlanetTerp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So that we can separate by major later\n",
    "merged_pt_bmgt_stats['major'] = 'bmgt'\n",
    "merged_pt_cmsc_stats['major'] = 'cmsc'\n",
    "\n",
    "merged_pt_bmgt_reviews['major'] = 'bmgt'\n",
    "merged_pt_cmsc_reviews['major'] = 'cmsc'\n",
    "\n",
    "all_pt_bmgt_grades['major'] = 'bmgt'\n",
    "all_pt_cmsc_grades['major'] = 'cmsc'\n",
    "\n",
    "all_pt_stats = merged_pt_bmgt_stats.append(merged_pt_cmsc_stats)\n",
    "all_pt_reviews = merged_pt_bmgt_reviews.append(merged_pt_cmsc_reviews)\n",
    "all_pt_grades = pd.concat([all_pt_cmsc_grades, all_pt_bmgt_grades])\n",
    "\n",
    "all_pt_stats['source'] = 'pt'\n",
    "all_pt_reviews['source'] = 'pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_pt_stats))\n",
    "all_pt_stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_pt_reviews))\n",
    "all_pt_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_pt_grades))\n",
    "all_pt_grades.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Across RateMyProfessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So that we can separate by major later\n",
    "merged_rmp_bmgt_stats['major'] = 'bmgt'\n",
    "merged_rmp_cmsc_stats['major'] = 'cmsc'\n",
    "\n",
    "# Get the grades for each RMP review from the RMP bmgt database\n",
    "\n",
    "merged_rmp_bmgt_reviews['major'] = 'bmgt'\n",
    "merged_rmp_cmsc_reviews['major'] = 'cmsc'\n",
    "\n",
    "\n",
    "# Grade data for BMGT reviews\n",
    "rmp_review_grades = get_rmp_data_for_all_ids(bmgt_rmp_db_filepath, merged_rmp_bmgt_reviews, get_rmp_review_grade)\n",
    "merged_rmp_bmgt_reviews = pd.merge(rmp_review_grades, merged_rmp_bmgt_reviews, on='review_id', how='outer')\n",
    "merged_rmp_bmgt_reviews.rename(columns={'value': 'expected_grade'}, inplace=True)\n",
    "\n",
    "# Grade data for CMSC reviews\n",
    "rmp_review_grades = get_rmp_data_for_all_ids(cmsc_rmp_db_filepath, merged_rmp_cmsc_reviews, get_rmp_review_grade)\n",
    "merged_rmp_cmsc_reviews = pd.merge(rmp_review_grades, merged_rmp_cmsc_reviews, on='review_id', how='outer')\n",
    "merged_rmp_cmsc_reviews.rename(columns={'value': 'expected_grade'}, inplace=True)\n",
    "\n",
    "all_rmp_stats = merged_rmp_bmgt_stats.append(merged_rmp_cmsc_stats)\n",
    "all_rmp_reviews = merged_rmp_bmgt_reviews.append(merged_rmp_cmsc_reviews)\n",
    "\n",
    "all_rmp_reviews['source'] = 'rmp'\n",
    "all_rmp_stats['source'] = 'rmp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_rmp_stats))\n",
    "all_rmp_stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_rmp_reviews))\n",
    "all_rmp_reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Across both RateMyProfessor and PlanetTerp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_stats_cols = ['full_name', 'page_exists', 'rating', 'major', 'source']\n",
    "all_stats = pd.concat([all_pt_stats[shared_stats_cols + ['review_count']], all_rmp_stats[shared_stats_cols + ['rating_count']]])\n",
    "\n",
    "# Combine the rating counts into a single column and sum the total\n",
    "all_stats.replace(np.nan, 0, inplace=True)\n",
    "all_stats['total_reviews'] = all_stats['review_count'] + all_stats['rating_count']\n",
    "all_stats.drop(['review_count', 'rating_count'], inplace=True, axis=1)\n",
    "\n",
    "# Average the rating given to the professor across both RMP and PT\n",
    "all_stats = all_stats.groupby(['full_name', 'major']).agg({'rating': 'mean', 'total_reviews': 'sum'}).reset_index()\n",
    "all_stats = all_stats[all_stats.total_reviews != 0]\n",
    "all_stats.rename(columns={'rating': 'avg_rating'}, inplace=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_stats))\n",
    "all_stats.head(22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_review_cols = ['full_name', 'course', 'date', 'body', 'major', 'rating',\n",
    "                      'expected_grade', 'sentiment_score',\n",
    "                      'sentiment_ground_label', 'source', 'review_id']\n",
    "\n",
    "all_reviews = pd.concat([all_pt_reviews[shared_review_cols], all_rmp_reviews[shared_review_cols]])\n",
    "\n",
    "# Combine the grades into a single column\n",
    "def remove_bad_grade_values(val):\n",
    "    if val not in grades_list:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return val\n",
    "    \n",
    "# TODO: Maybe could keep the non-grade values and be used another way?\n",
    "all_reviews['expected_grade'] = all_reviews['expected_grade'].map(remove_bad_grade_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_reviews))\n",
    "all_reviews.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO: Put together majors as well?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cmsc_stats = all_stats[all_stats['major'] == 'cmsc']\n",
    "all_cmsc_reviews = all_reviews[all_reviews['major'] == 'cmsc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_cmsc_stats))\n",
    "all_cmsc_stats.head(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_cmsc_reviews))\n",
    "all_cmsc_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_bmgt_stats = all_stats[all_stats['major'] == 'bmgt']\n",
    "all_bmgt_reviews = all_reviews[all_reviews['major'] == 'bmgt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_bmgt_stats))\n",
    "all_bmgt_stats.head(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_bmgt_reviews))\n",
    "all_bmgt_reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Describe section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a recap we have the following dataframes at our disposal:\n",
    "* all_stats, all_reviews - Combination of majors and RMP with PT stats and reviews\n",
    "    * Stats headers: professor name, major, average rating, total reviews\n",
    "    * Review headers: profesor name, major, course id, date (unix), body of the review, and the reviewer's grade\n",
    "* all_bmgt_stats, all_cmsc_stats - Combined professor statistics from both RMP and PT\n",
    "* all_bmgt_reviews, all_cmsc_reviews - Combined introductory course reviews from both RMP and PT\n",
    "\n",
    "* all_rmp_stats, all_rmp_reviews - Combined CMSC and BMGT stats and reviews from RMP\n",
    "    * Stats headers: first/last/full name of professor, major, their average quality rating, difficulty rating, rating count\n",
    "    * Review headers: review id, first/last/full name of professor, major, course id, date (unix), body of the review, quality rating, difficulty rating, thumb up/down\n",
    "* all_rmp_bmgt_stats, all_rmp_cmsc_stats - Professor statistics from RMP for both BMGT and CMSC\n",
    "* all_rmp_bmgt_reviews, all_rmp_cmsc_reviews - Course reviews for each major from PT\n",
    "\n",
    "* all_pt_stats, all_pt_reviews - Combined CMSC and BMGT professor stats and reviews from PT\n",
    "    * Stats headers: first/last/full name of professor, average rating, review count, type of faculty, and major\n",
    "    * Review headers: review id, full name of professor, course, date (unix), body of the review, rating, expected grade, and major\n",
    "* all_pt_bmgt_stats, all_pt_cmsc_stats - Professor statistics for each major from PT\n",
    "* all_pt_bmgt_reviews, all_pt_cmsc_reviews - Course reviews for each major from PT\n",
    "\n",
    "* all_pt_grades - Combined grades from the majors, data from PT\n",
    "    * Headers: course, semester, count of the amount per grade (A+ through W), and major\n",
    "* all_pt_cmsc_grades, all_pt_bmgt_grades - \n",
    "    * Headers: course, semester, count of the amount per grade (A+ through W)\n",
    "    \n",
    "Note the combined major dataframes have a 'major' column, which is the only difference between the separated dataframes' columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_stats.columns)\n",
    "print(all_reviews.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_rmp_stats.columns)\n",
    "print(all_rmp_reviews.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_pt_stats.columns)\n",
    "print(all_pt_reviews.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_pt_grades.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fall start ~August 20 and after\n",
    "fall_start = datetime.datetime(year=2020, month=8, day=20)\n",
    "# Winter start ~January 2nd\n",
    "winter_start = datetime.datetime(year=2020, month=1, day=2)\n",
    "# Spring start ~January 20th\n",
    "spring_start = datetime.datetime(year=2020, month=1, day=20)\n",
    "# Summer start ~June 1st\n",
    "summer_start = datetime.datetime(year=2020, month=6, day=1)\n",
    "\n",
    "def unix_to_semester(timestamp):\n",
    "    review_date = datetime.datetime.utcfromtimestamp(timestamp)\n",
    "    review_year = review_date.year\n",
    "    \n",
    "    if review_date.month >= fall_start.month:\n",
    "        return 'Fall'\n",
    "    elif review_date.month >= summer_start.month:\n",
    "        return 'Summer'\n",
    "    elif review_date.month >= spring_start.month and review_date.day >= spring_start.day:\n",
    "        return 'Spring'\n",
    "    else:\n",
    "        return 'Winter'\n",
    "    \n",
    "    return None\n",
    "\n",
    "def get_month_from_unix(timestamp):\n",
    "    date = datetime.datetime.utcfromtimestamp(timestamp)\n",
    "    \n",
    "    return date.strftime('%B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the Breadth of Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stat and Review Count Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrames that contain the counts of each major's reviews and stats\n",
    "stats_df = pd.DataFrame(columns=['major', 'count'])\n",
    "stats_df['major'] = ['CMSC', 'BMGT']\n",
    "stats_df['count'] = [len(all_cmsc_stats), len(all_bmgt_stats)]\n",
    "\n",
    "reviews_df = pd.DataFrame(columns=['major', 'count'])\n",
    "reviews_df['major'] = ['CMSC', 'BMGT1']\n",
    "reviews_df['count'] = [len(all_cmsc_reviews), len(all_bmgt_reviews)]\n",
    "\n",
    "# Plot them out to visually compare\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "fig.suptitle('Comparing CMSC to BMGT in Professor and Review Count')\n",
    "ax[0].set(title=\"Review Count Comparison\")\n",
    "ax[1].set(title=\"Professor Count Comparison\")\n",
    "\n",
    "sns.barplot(x='major', y='count', data=reviews_df, ax=ax[0])\n",
    "sns.barplot(x='major', y='count', data=stats_df, ax=ax[1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at Grade Dispersion for Each Major"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades_df = pd.DataFrame()\n",
    "\n",
    "# Get the grades from all the reviews, dropping reviews without a grade\n",
    "grades_df = all_reviews[['major', 'expected_grade']].copy().dropna()\n",
    "\n",
    "# Count how many of each grade per major\n",
    "grades_df['count'] = 0\n",
    "agg_grades_df = grades_df.groupby(['major', 'expected_grade']).count().reset_index()\n",
    "\n",
    "# How many grades total do we have from each major\n",
    "total_bmgt_grades = agg_grades_df.loc[agg_grades_df['major'] == 'bmgt']['count'].sum()\n",
    "total_cmsc_grades = agg_grades_df.loc[agg_grades_df['major'] == 'cmsc']['count'].sum()\n",
    "\n",
    "# Separate out the DataFrame so that it'll be easier to update values\n",
    "bmgt_grades_df = agg_grades_df.loc[agg_grades_df['major'] == 'bmgt'].copy()\n",
    "cmsc_grades_df = agg_grades_df.loc[agg_grades_df['major'] == 'cmsc'].copy()\n",
    "\n",
    "cmsc_grades_df.update(cmsc_grades_df['count'].divide(total_cmsc_grades))\n",
    "bmgt_grades_df.update(bmgt_grades_df['count'].divide(total_bmgt_grades))\n",
    "\n",
    "# Bring them back together with the percentage values\n",
    "agg_grades_df = pd.concat([cmsc_grades_df, bmgt_grades_df])\n",
    "\n",
    "# Plot out the grade dispersion per major\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "sns.barplot(ax=ax, data=agg_grades_df, x='expected_grade', y='count', hue='major', order=grades_list)\n",
    "\n",
    "ax.set(title=\"Grade Distribution of Majors\", xlabel=\"Grade\", ylabel=\"Percentage of Grade\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at Grade Dispersion Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "grade_time_df = all_reviews[['expected_grade', 'date', 'major']].copy().dropna()\n",
    "grade_time_df['month'] = grade_time_df['date'].apply(lambda date: get_month_from_unix(date))\n",
    "\n",
    "grade_month_major = grade_time_df.groupby(['month', 'major'])['expected_grade'].count().reset_index()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 5))\n",
    "sns.barplot(ax=ax, data=grade_month_major, x='month', y='expected_grade', hue='major')\n",
    "\n",
    "ax.set(title=\"Reported Grades per Month\", xlabel='', ylabel='Grades Reported')\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Word Frequencies and Rudimentary Sentiment Analysis in Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Need to run python -m spacy download en_core_web_sm (also a medium and large dataset)\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "punctuation = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credit to https://www.dataquest.io/blog/tutorial-text-classification-in-python-using-spacy/\n",
    "def tokenize_lemmatize_text(text):\n",
    "    tokens = nlp(text)\n",
    "    \n",
    "    tokens = [word.lemma_.lower().strip() if word.lemma_ != '-PRON-' else word.lower_ for word in tokens]\n",
    "    tokens = [lemma for lemma in tokens if lemma not in spacy_stopwords and lemma not in punctuation]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Word Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bag of Words maybe for TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(tokenizer=tokenize_lemmatize_text, ngram_range=(1,1))\n",
    "X = vectorizer.fit_transform([good_text, bad_text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label the Reviews as Positive or Negative - Rudimentary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Explain why we did it this way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading Positive and Negative Lexicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_word_dict(filepath, word_dict):\n",
    "    \"\"\"Saves word frequency dictionaries to a pickle file.\"\"\"\n",
    "    pickle.dump(word_dict, open(filepath, 'wb'))\n",
    "    \n",
    "def read_word_dict_file(filepath):\n",
    "    \"\"\"Reads a word frequency dictionary from a pickle file.\"\"\"\n",
    "    return pickle.load(open(filepath, 'rb'))\n",
    "\n",
    "def read_lexicon(filepath):\n",
    "    \"\"\"Read a lexicon full of words from a file and insert them into\n",
    "    a list to return. Simple \\n separation.\n",
    "    \n",
    "    Args:\n",
    "        filepath: A string containing the filepath to the lexicon.\n",
    "        \n",
    "    Returns:\n",
    "        A list of words (one per line).\n",
    "    \"\"\"\n",
    "    \n",
    "    lexicon = []\n",
    "    \n",
    "    with open(filepath, 'r') as fp:\n",
    "        all_words = fp.readlines()\n",
    "        for word in all_words:\n",
    "            lexicon.append(word.strip())\n",
    "            \n",
    "    return lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load positive words\n",
    "positive_words = read_lexicon('./data/lexicon/positive-words.txt')\n",
    "positive_words.append('funny')\n",
    "positive_words.remove('tough')\n",
    "\n",
    "# Load negative words\n",
    "negative_words = read_lexicon('./data/lexicon/negative-words.txt')\n",
    "negative_words.append('tough')\n",
    "negative_words.remove('funny')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions for Scoring and Labeling the Sentiment of Reviews "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_sentiment_score_labels(review_df, source, major):\n",
    "    if not should_store_data:\n",
    "        return\n",
    "    \n",
    "    if source == 'pt':\n",
    "        if major == 'bmgt':\n",
    "            db_filepath = bmgt_pt_db_filepath\n",
    "        else:\n",
    "            db_filepath = cmsc_pt_db_filepath\n",
    "    else:\n",
    "        if major == 'bmgt':\n",
    "            db_filepath = bmgt_rmp_db_filepath\n",
    "        else:\n",
    "            db_filepath = cmsc_rmp_db_filepath\n",
    "            \n",
    "    db_conn = create_connection(db_filepath)\n",
    "    \n",
    "    try:\n",
    "        insert_all_review_sentiment_labels(db_conn, review_df)\n",
    "    except Exception as e:\n",
    "        print(\"Type error: \" + str(e))\n",
    "        traceback.print_exc()\n",
    "        \n",
    "    finally:\n",
    "        if db_conn:\n",
    "            db_conn.close()\n",
    "            \n",
    "            \n",
    "def update_word_dict(word_dict, word):\n",
    "    \"\"\"Updates a counting dictionary depending on if key is present.\"\"\"\n",
    "    if word not in word_dict:\n",
    "        word_dict[word] = 1\n",
    "    else:\n",
    "        word_dict[word] = word_dict[word] + 1\n",
    "        \n",
    "    return word_dict\n",
    "\n",
    "def normalize_score(score, max_score, min_score):\n",
    "    \"\"\"Normalizes the score to [-1, 1] range.\"\"\"\n",
    "    denom = (max_score - min_score)\n",
    "    \n",
    "    if denom == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    norm_score = 2 * ((score - min_score) / denom) - 1\n",
    "    \n",
    "    return norm_score\n",
    "\n",
    "def get_label_from_score(score):\n",
    "    \"\"\"Returns a label for a score, where 1 is positive,\n",
    "    0 is neutral, and -1 is negative.\"\"\"\n",
    "    \n",
    "    if score > 0:\n",
    "        return 1\n",
    "    elif score < 0:\n",
    "        return -1\n",
    "    \n",
    "    return score\n",
    "        \n",
    "def score_review(review_body, positive_dict, negative_dict, all_word_dict):\n",
    "    \"\"\"A rudimentary sentiment scoring algorithm, which subtracts from the\n",
    "    score when a negative word is present and adds to the score when a \n",
    "    positive word is present. No POS is used. Words are lemmatized.\n",
    "    \n",
    "    Args:\n",
    "        review_body: A string containing the body of the review.\n",
    "        \n",
    "    Returns:\n",
    "        An integer score\n",
    "    \"\"\"\n",
    "    \n",
    "    score = 0\n",
    "    tokens = tokenize_lemmatize_text(review_body)\n",
    "    \n",
    "    for token in tokens:\n",
    "        all_word_dict = update_word_dict(all_word_dict, token)\n",
    "        \n",
    "        if token in positive_words:\n",
    "            positive_dict = update_word_dict(positive_dict, token)\n",
    "            score = score + 1\n",
    "        \n",
    "        elif token in negative_words:\n",
    "            negative_dict = update_word_dict(negative_dict, token)\n",
    "            score = score - 1\n",
    "            \n",
    "    return (score, positive_dict, negative_dict, all_word_dict)\n",
    "\n",
    "def score_all_reviews(review_df):\n",
    "    \"\"\"Scores each review as in range [-1, 1], and also keeps\n",
    "    track of the frequencies of words for all, positive, and negative words.\n",
    "    \n",
    "    Args:\n",
    "        review_df: A dataframe containing reviews.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple of an updates review dataframe with scores, a positive word frequency dictionary,\n",
    "        a negative word frequency dictionary, and all word dictionary frequency.\n",
    "    \"\"\"\n",
    "    \n",
    "    scores = []\n",
    "    positive_dict = {}\n",
    "    negative_dict = {}\n",
    "    all_word_dict = {}\n",
    "    \n",
    "    for row in review_df.itertuples():\n",
    "        (score, positive_dict, negative_dict, all_word_dict) = score_review(row.body, positive_dict, negative_dict, all_word_dict)\n",
    "        scores.append(score)\n",
    "        \n",
    "    # TODO: Normalize scores if we want a range of positive/negativeness?\n",
    "    max_score = max(scores)\n",
    "    min_score = min(scores)\n",
    "    norm_scores = [normalize_score(score, max_score, min_score) for score in scores]\n",
    "    \n",
    "    # Label the reviews\n",
    "    labels = [get_label_from_score(score) for score in scores]\n",
    "    \n",
    "    review_df['sentiment_score'] = scores\n",
    "    review_df['sentiment_ground_label'] = labels\n",
    "    \n",
    "    return (review_df, positive_dict, negative_dict, all_word_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Score and Label CMSC Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmsc_dict_fp = './data/word_lists/cmsc_'\n",
    "\n",
    "cmsc_review_sentiment_df = all_cmsc_reviews.copy()\n",
    "null_sentiment_labels = cmsc_review_sentiment_df['sentiment_ground_label'].isnull().sum()\n",
    "\n",
    "if null_sentiment_labels == len(cmsc_review_sentiment_df):\n",
    "    (cmsc_review_sentiment_df, cmsc_positive_dict, cmsc_negative_dict, cmsc_all_word_dict) = score_all_reviews(cmsc_review_sentiment_df)\n",
    "    \n",
    "    cmsc_pt_reviews = cmsc_review_sentiment_df.loc[cmsc_review_sentiment_df['source'] == 'pt']\n",
    "    save_sentiment_score_labels(cmsc_pt_reviews, 'pt', 'cmsc')\n",
    "        \n",
    "    cmsc_rmp_reviews = cmsc_review_sentiment_df.loc[cmsc_review_sentiment_df['source'] == 'rmp']\n",
    "    save_sentiment_score_labels(cmsc_rmp_reviews, 'rmp', 'cmsc')\n",
    "    \n",
    "    save_word_dict(cmsc_dict_fp + 'positive.p', cmsc_positive_dict)\n",
    "    save_word_dict(cmsc_dict_fp + 'negative.p', cmsc_negative_dict)\n",
    "    save_word_dict(cmsc_dict_fp + 'all.p', cmsc_all_word_dict)\n",
    "else:\n",
    "    cmsc_positive_dict = read_word_dict_file(cmsc_dict_fp + 'positive.p')\n",
    "    cmsc_negative_dict = read_word_dict_file(cmsc_dict_fp + 'negative.p')\n",
    "    cmsc_all_word_dict = read_word_dict_file(cmsc_dict_fp + 'all.p')\n",
    "\n",
    "cmsc_review_sentiment_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Score and Label BMGT Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bmgt_dict_fp = './data/word_lists/bmgt_'\n",
    "\n",
    "bmgt_review_sentiment_df = all_bmgt_reviews.copy()\n",
    "null_sentiment_labels = bmgt_review_sentiment_df['sentiment_ground_label'].isnull().sum()\n",
    "\n",
    "if null_sentiment_labels == len(bmgt_review_sentiment_df): \n",
    "    (bmgt_review_sentiment_df, bmgt_positive_dict, bmgt_negative_dict, bmgt_all_word_dict) = score_all_reviews(bmgt_review_sentiment_df)\n",
    "    \n",
    "    bmgt_pt_reviews = bmgt_review_sentiment_df.loc[bmgt_review_sentiment_df['source'] == 'pt']\n",
    "    save_sentiment_score_labels(bmgt_pt_reviews, 'pt', 'bmgt')\n",
    "        \n",
    "    bmgt_rmp_reviews = bmgt_review_sentiment_df.loc[bmgt_review_sentiment_df['source'] == 'rmp']\n",
    "    save_sentiment_score_labels(bmgt_rmp_reviews, 'rmp', 'bmgt')\n",
    "    \n",
    "    save_word_dict(bmgt_dict_fp + 'positive.p', bmgt_positive_dict)\n",
    "    save_word_dict(bmgt_dict_fp + 'negative.p', bmgt_negative_dict)\n",
    "    save_word_dict(bmgt_dict_fp + 'all.p', bmgt_all_word_dict)\n",
    "else:\n",
    "    bmgt_positive_dict = read_word_dict_file(bmgt_dict_fp + 'positive.p')\n",
    "    bmgt_negative_dict = read_word_dict_file(bmgt_dict_fp + 'negative.p')\n",
    "    bmgt_all_word_dict = read_word_dict_file(bmgt_dict_fp + 'all.p')\n",
    "    \n",
    "\n",
    "bmgt_review_sentiment_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at how often these types of words are used per major"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmsc_all_word_count = sum(cmsc_all_word_dict.values())\n",
    "cmsc_positive_pct = sum(cmsc_positive_dict.values()) / cmsc_all_word_count\n",
    "cmsc_negative_pct = sum(cmsc_negative_dict.values()) / cmsc_all_word_count\n",
    "\n",
    "bmgt_all_word_count = sum(bmgt_all_word_dict.values())\n",
    "bmgt_positive_pct = sum(bmgt_positive_dict.values()) / bmgt_all_word_count\n",
    "bmgt_negative_pct = sum(bmgt_negative_dict.values()) / bmgt_all_word_count\n",
    "\n",
    "word_pct_df = pd.DataFrame(columns=['type', 'major', 'percentage'])\n",
    "\n",
    "word_pct_df = word_pct_df.append({'type': 'positive' , 'major': 'bmgt', 'percentage': bmgt_positive_pct}, ignore_index=True)\n",
    "word_pct_df = word_pct_df.append({'type': 'negative' , 'major': 'bmgt', 'percentage': bmgt_negative_pct}, ignore_index=True)\n",
    "\n",
    "\n",
    "word_pct_df = word_pct_df.append({'type': 'positive' , 'major': 'cmsc', 'percentage': cmsc_positive_pct}, ignore_index=True)\n",
    "word_pct_df = word_pct_df.append({'type': 'negative' , 'major': 'cmsc', 'percentage': cmsc_negative_pct}, ignore_index=True)\n",
    "\n",
    "\n",
    "ax = sns.catplot(x='type', y='percentage', kind='bar', hue='major', data=word_pct_df)\n",
    "ax.set(xlabel='Type of words', ylabel='Perctange of all words', title='Percentage of types of words per major')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Review Trends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Describe section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Word Associations and Sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wordcloud(word_freq_dict):\n",
    "    \"\"\"Creates a single WordCloud given the word frequency dictionary.\n",
    "    \n",
    "    Args:\n",
    "        word_freq_dict: A dictionary of word frequencies.\n",
    "        \n",
    "    Returns:\n",
    "        A WordCloud object created from the provided dictionary.\n",
    "    \"\"\"\n",
    "    \n",
    "    wc = WordCloud(background_color=\"white\", max_words=100)\n",
    "    wc.generate_from_frequencies(word_freq_dict)\n",
    "    \n",
    "    return wc\n",
    "    \n",
    "def plot_multiple_wordclouds(titles, word_freq_dicts):\n",
    "    \"\"\"Creates a wordcloud for each word frequency dictionary into a \n",
    "    3 x 1 subplot figure. Titles must be the same length as the dictionaries.\n",
    "    \n",
    "    Args:\n",
    "        titles: Title of each plot/word cloud.\n",
    "        word_freq_dicts: List of word frequency dictionaries.\n",
    "    \"\"\"\n",
    "    \n",
    "    figure, ax = plt.subplots(1, 3, figsize=(30, 10))\n",
    "    \n",
    "    for idx in range(len(word_freq_dicts)):\n",
    "        ax[idx].imshow(create_wordcloud(word_freq_dicts[idx]))\n",
    "        ax[idx].axis('off')\n",
    "        ax[idx].set_title(titles[idx], fontdict={'fontsize': 23})\n",
    "            \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_multiple_wordclouds(['Positive', 'Negative', 'All'], [cmsc_positive_dict, cmsc_negative_dict, cmsc_all_word_dict])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_multiple_wordclouds(['Positive', 'Negative', 'All'], [bmgt_positive_dict, bmgt_negative_dict, bmgt_all_word_dict])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Review Trends and Trends Over Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights and Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is worthy to consider possible sources of bias inside our data set. One bias at play is self-selection bias -- the bias that people who have had negative experiences are more likely to respond on surveys or polls. In the case of RateMyProfessor, this is almost certainly influential -- students who received their first bad grade, or first conflict with a professor, are more likely to express and vent their frustration than students who have had relatively plain or positive experiences. Furthermore, the aggregate of all reviews from each major yielded about 2x more CS students than BMGT students (726 CS to 387 BMGT) -- for one reason or another, CS students are more represented on these online websites (participation bias). Although we normalized for these (accounting for percentages of reviews positive/negative, rather than total counts), the sample size may not be perfectly representative of the population as a whole. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example of Self-Selection Bias\n",
    "from IPython.display import Image\n",
    "Image(\"img/angry_review.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Potential Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Future research can be divided into two categories, (1) improving the accuracy, models, and analysis of this current work, or (2) expanding the scope of the research, seeking to answer broader questions.\n",
    "\n",
    "Improvements could seek to gage how representative the PlanetTerp reviews are, by comparing them to the actual course data recorded by UMD (although this would require special permissions to access data about course grade distributions in the past). Furthermore, improvements to the NLP model can be made -- our training data set was rudimentary in its generation, by simply counting positive/negative word frequencies -- a deeper, more accurate model could increase overall accuracy and validity of our training data before it enters the Machine Learning phase. \n",
    "\n",
    "Ideas for expansion could consider more majors at UMD, and seek to answer questions about more general trends -- how do STEM majors compare in course reviews to those of Humanities subjects? Which professors are the \"most likable\" in all of UMD? Eventually, once the models are good enough, they can even be extracted to consider other Universities with comparatively little extra work -- just employ the same models on similar websites (RateMyProfessor techniques and models can be recycled, as the website stores data for most universities). How do the mean grades of introductory CS classes at UMD compare to those of CS classes at UC Berkeley? Are CS students overrepresented at those campuses as well? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
